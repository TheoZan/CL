{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info\n",
    "\n",
    "* discrete action space of size 21, 10 discharge, 10 charge, 1 noop\n",
    "* only One building\n",
    "* only one battery\n",
    "* can use custom reward with zeta parameter\n",
    "* can yous masked action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "import citylearn\n",
    "from citylearn.energy_model import HeatPump\n",
    "from citylearn.utilities import read_json\n",
    "\n",
    "import gym\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from stable_baselines3 import PPO, A2C, DDPG, TD3, SAC\n",
    "\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    building_info = env.get_building_information()\n",
    "    building_info = list(building_info)\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "                \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingDevices:\n",
    "  \"\"\"\n",
    "    Keeps track of all storage devices of a building.\n",
    "  \"\"\"\n",
    "  def __init__(self, building, num_building):\n",
    "    self.num_building = num_building\n",
    "    self.building = building\n",
    "    self.devices = {'battery' : Device_(building.electrical_storage, 'battery'),\n",
    "                    'cooling' : None,\n",
    "                    'dhw' : None}\n",
    "    \n",
    "  def compute_bounds(self):\n",
    "    bounds = [self.bounds_action(i) for i,j in self.devices.items() if j is not None]\n",
    "    return gym.spaces.Box(low=np.array([i[0] for i in bounds]), high=np.array([i[1] for i in bounds]), dtype=np.float64)\n",
    "  \n",
    "# ACTION 0 :  cooling\n",
    "# ACTION 1 : dhw\n",
    "# ACTION 2 : battery\n",
    "  \n",
    "  def bounds_action(self, type_action):\n",
    "    device = self.devices[type_action].device\n",
    "    if device is None:\n",
    "        return None # if return none building doest have battery\n",
    "    if type_action == 'battery':\n",
    "        capacity = device.capacity_history[-2] if len(device.capacity_history) > 1 else device.capacity\n",
    "        #HIGH\n",
    "        #get max energy that the storage unit can use to charge [kW]\n",
    "        #if trying to put more than the battery can accept reject action\n",
    "        high1 = device.get_max_input_power()/capacity\n",
    "        high2 = (device.capacity - device.soc_init)/(0.95*device.capacity) #approxim (efficiency = 0.95)\n",
    "        high = min(high1, high2, 1)\n",
    "\n",
    "        #LOW\n",
    "        low1 = -device.get_max_input_power()/capacity\n",
    "        low2 = (-device.soc_init*0.95)/device.capacity #approxim (efficiency = 0.95)\n",
    "        low = max(low1, low2, -1)\n",
    "\n",
    "    else:\n",
    "        bool_h2, bool_l2 = False, False\n",
    "        if type_action == 'cooling':\n",
    "            # print('\\ncooling')\n",
    "            space_demand = self.building.cooling_demand[self.building.time_step]\n",
    "            max_output = self.building.cooling_device.get_max_output_power(self.building.weather.outdoor_dry_bulb_temperature[self.building.time_step], False)\n",
    "            # print('space_demand',space_demand)\n",
    "            # print('max_output', max_output)\n",
    "            # print('capacity:', device.capacity)\n",
    "        else: #dhw\n",
    "            # print('\\ndhw')\n",
    "            space_demand = self.building.dhw_demand[self.building.time_step]\n",
    "            max_output = self.building.dhw_device.get_max_output_power(self.building.weather.outdoor_dry_bulb_temperature[self.building.time_step], False)\\\n",
    "            if isinstance(self.building.dhw_device, HeatPump) else self.building.dhw_device.get_max_output_power()\n",
    "            # print('space_demand',space_demand)\n",
    "            # print('max_output', max_output)\n",
    "        space_demand = 0 if space_demand is None or math.isnan(space_demand) else space_demand # case where space demand is unknown\n",
    "\n",
    "        #HIGH\n",
    "        high1 = (max_output-space_demand) / device.capacity\n",
    "        # print('high1', high1)\n",
    "        if device.max_input_power is not None:\n",
    "            bool_h2 = True\n",
    "            high2 = device.max_input_power / device.capacity\n",
    "            # print('high2', high2)\n",
    "        high3 = (device.capacity - device.soc_init) / (device.capacity*device.efficiency)\n",
    "        # print(device.capacity, device.soc_init)\n",
    "        # print('high3', high3)\n",
    "        \n",
    "        if bool_h2:\n",
    "            high = min(high1, high2, high3, 0.5)\n",
    "        else:\n",
    "            high = min(high1, high3, 0.5)\n",
    "\n",
    "\n",
    "        #LOW\n",
    "        low1 = -space_demand / device.capacity\n",
    "        # print('low1', low1)\n",
    "        if device.max_output_power is not None:\n",
    "            bool_l2 = True\n",
    "            low2 = -device.max_output_power / device.capacity\n",
    "            # print('low2',low2)\n",
    "        low3 = (-device.soc_init*device.efficiency) / device.capacity\n",
    "        # print('low3',low3)\n",
    "\n",
    "        if bool_l2:\n",
    "            low = max(low1, low2, low3, -0.5)\n",
    "        else:\n",
    "            low = max(low1, low3, -0.5)\n",
    "\n",
    "    return (low, high)\n",
    "  \n",
    "  def cost(self, zeta):\n",
    "    \"\"\"\n",
    "    Other way to compute cost.\n",
    "    1) we compute the total electrical consumption of the building,\n",
    "    2) we the offset the PV generation if existant.\n",
    "    3) we treat the case of charging and discharging the device \n",
    "    \"\"\"\n",
    "    #without dhw and cooling storage\n",
    "    #net conso = cooling + dhw + electrical_storage + nsl - solar\n",
    "    global_conso = 0\n",
    "    building = self.building\n",
    "\n",
    "    price = building.pricing.electricity_pricing[building.time_step]\n",
    "    carbon = building.carbon_intensity.carbon_intensity[building.time_step]\n",
    "\n",
    "    # print(building.time_step)\n",
    "    cooling_demand = building.energy_simulation.cooling_demand[building.time_step] + building.cooling_storage.energy_balance[building.time_step]\n",
    "    cooling_conso = building.cooling_device.get_input_power(cooling_demand, building.weather.outdoor_dry_bulb_temperature[building.time_step], heating=False)\n",
    "    global_conso += cooling_conso\n",
    "\n",
    "    dhw_demand = building.energy_simulation.dhw_demand[building.time_step] + building.dhw_storage.energy_balance[building.time_step]\n",
    "    if isinstance(building.dhw_device, HeatPump):\n",
    "            dhw_consumption = building.dhw_device.get_input_power(dhw_demand, building.weather.outdoor_dry_bulb_temperature[building.time_step], heating=True)\n",
    "    else:\n",
    "            dhw_consumption = building.dhw_device.get_input_power(dhw_demand)\n",
    "    \n",
    "    global_conso += dhw_consumption\n",
    "    global_conso += building.energy_simulation.non_shiftable_load[building.time_step]\n",
    "    global_conso -= building.pv.get_generation(building.energy_simulation.solar_generation)[building.time_step]\n",
    "\n",
    "    #battery\n",
    "    #discharge \n",
    "    battery_conso = building.energy_from_electrical_storage[building.time_step] * (1 - zeta)\n",
    "    #charge\n",
    "    energy_used = building.energy_to_electrical_storage[building.time_step]\n",
    "    global_conso += energy_used * zeta\n",
    "    if energy_used > 0: #charging\n",
    "        self.devices['battery'].update_cost(energy_used, price, carbon)\n",
    "\n",
    "    # print(global_conso)\n",
    "    cost = (price + carbon) * global_conso\n",
    "    cost += self.devices['battery'].cost * battery_conso\n",
    "\n",
    "    return -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Device_:\n",
    "  def __init__(self, device, storage_type):\n",
    "    self.device = device\n",
    "    # self.price_cost = 0\n",
    "    # self.emission_cost = 0\n",
    "    self.cost = 0\n",
    "    self.storage_type = storage_type\n",
    "\n",
    "  def loss(self, cost_t, pv_offset, battery_offset):\n",
    "    \"\"\"\n",
    "    get avg price between (battery release, grid release and PV- direct consumption)\n",
    "    add relative incertainty, but true in pratice as the energy is added up in a global consumption pool \n",
    "\n",
    "    battery: if battery releases, price = avg((total released by battery - remaining conso), grid) in the case of thermal\n",
    "    in the case of battery, avg price with PV\n",
    "    \"\"\"\n",
    "    if not self.device:\n",
    "      print('not device')\n",
    "      raise ValueError\n",
    "\n",
    "    energy_used = self.device.energy_balance[-1]\n",
    "    if isinstance(energy_used, np.ndarray):\n",
    "      print('probleme energy used array instead of float')\n",
    "      energy_used = energy_used[0]\n",
    "\n",
    "    #charge\n",
    "    if energy_used > 0:\n",
    "      #if pv production, part of the energy is free\n",
    "      if pv_offset > 0:\n",
    "        energy_used = max(0, energy_used-pv_offset)\n",
    "      #if usage of battery, part of energy has been already taken into account so free\n",
    "      if battery_offset > 0:\n",
    "        energy_used = max(0, energy_used-battery_offset)\n",
    "      # self.price_cost = ((self.price_cost*self.device.soc[-2])+(energy_used*price))/self.device.soc[-1]\n",
    "      # self.emission_cost = ((self.emission_cost*self.device.soc[-2])+(energy_used*emission))/self.device.soc[-1]\n",
    "      \n",
    "      total = self.device.soc[-1]\n",
    "      if isinstance(total, np.ndarray):\n",
    "        print('probleme soc-1 array instead of float')\n",
    "        total = total[0]\n",
    "\n",
    "      prev = self.device.soc[-2]\n",
    "      if isinstance(prev, np.ndarray):\n",
    "        print('probleme soc-2 array instead of float')\n",
    "        prev = prev[0]\n",
    "\n",
    "      self.cost = ((self.cost*prev) + (energy_used*cost_t)) / total\n",
    "      return energy_used, None, None #energy_used > 0\n",
    "\n",
    "    #discharge\n",
    "    else:\n",
    "      #energy_processed is total energy used during charge/discharge process including losses\n",
    "      #energy_used is the energy_processed minus the losses (used by building)\n",
    "      energy_processed = self.device.soc[-2]-self.device.soc[-1]\n",
    "      return -energy_used, energy_processed, self.cost # -energy_used > 0, energy_processed > 0 \n",
    "\n",
    "  def update_cost(self, energy_used, price_t, emission_t):\n",
    "    prev_soc = 0 if len(self.device.soc)<2 else self.device.soc[-2]\n",
    "    cost_t = price_t + emission_t\n",
    "    self.cost = ((self.cost*prev_soc) + (energy_used*cost_t)) / self.device.soc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset(building, mode):\n",
    "  \"\"\"\n",
    "  building is env.buildings[i]:\n",
    "  mode = 'pv' or 'battery'\n",
    "\n",
    "  each conso gets an equally distributed offset based on solar generation or battery\n",
    "  discharge\n",
    "  \"\"\"\n",
    "  if mode == 'pv':\n",
    "    if not building.solar_generation is None:\n",
    "      return 0\n",
    "    demands = [building.non_shiftable_load_demand[-2], building.electrical_storage.energy_balance[-1],\n",
    "             building.dhw_demand[-2], building.dhw_storage.energy_balance[-1],\n",
    "             building.cooling_demand[-2], building.cooling_storage.energy_balance[-1]]\n",
    "    count = len([i for i in demands if i > 0])\n",
    "    return -building.solar_generation[-2]/count\n",
    "  else:\n",
    "    if not building.solar_generation is None:\n",
    "      return 0\n",
    "    if building.electrical_storage.energy_balance[-1] >=0:\n",
    "      return 0\n",
    "    demands = [building.non_shiftable_load_demand[-2], building.dhw_demand[-2],\n",
    "            building.dhw_storage.energy_balance[-1], building.cooling_demand[-2],\n",
    "             building.cooling_storage.energy_balance[-1]]\n",
    "    count = len([i for i in demands if i > 0])\n",
    "    return -building.electrical_storage.energy_balance[-1]/count\n",
    "\n",
    "def compute_loss(building, building_devices, price, emission, outdoor_dry_bulb_temperature, zeta):\n",
    "  loss = 0\n",
    "  pv_offset = get_offset(building, 'pv') \n",
    "  battery_offset = get_offset(building, 'battery')\n",
    "  # print('pv offset',pv_offset)\n",
    "  # print('battery_offset', battery_offset)\n",
    "\n",
    "  #1) compute loss for storage devices use or update cost in storage\n",
    "  for name,device in building_devices.devices.items():\n",
    "    #if the device exists in building\n",
    "    if device:\n",
    "      energy_used, energy_processed, cost = device.loss(price*emission, pv_offset, battery_offset)\n",
    "    #else consider it exists and set energy used = 0\n",
    "    #so we can compute the remaining demand associated with the device\n",
    "    else:\n",
    "      energy_used = 0\n",
    "\n",
    "    if not energy_processed: #charge\n",
    "      #account for a part of the cost at charging time\n",
    "      loss += (price * emission) * (energy_used * zeta)\n",
    "    else: #discharge\n",
    "      loss += cost * (energy_processed * (1 - zeta))\n",
    "\n",
    "    #2) compute remaining thermal demand and add cost of remaining direct demand to answer\n",
    "    if name == 'cooling':\n",
    "      #cooling and dhw stored energy is thermal not electrical\n",
    "      remaining = building.cooling_demand[-2] - energy_used\n",
    "      # print('remaining', remaining)\n",
    "      if remaining > 0:\n",
    "        energy = max(0, building.cooling_device.get_input_power(remaining, outdoor_dry_bulb_temperature, False) - pv_offset - battery_offset)\n",
    "        # print('energy', energy)\n",
    "        loss += (price + emission) * energy\n",
    "\n",
    "    elif name == 'dhw':\n",
    "      remaining = building.dhw_demand[-2]\n",
    "      # print('remaining', remaining)\n",
    "      if remaining > 0:\n",
    "        energy = max(0, building.dhw_device.get_input_power(remaining) - pv_offset - battery_offset)\n",
    "        # print('energy', energy)\n",
    "        loss += (price + emission) * energy\n",
    "\n",
    "  #3) compute additionnal loss coming from nsl\n",
    "  nsl = max(0, building.non_shiftable_load_demand[-2] - pv_offset - battery_offset)\n",
    "  loss += (price + emission) * nsl\n",
    "  # print(loss)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvCityGym(gym.Env):\n",
    "    \"\"\"\n",
    "    Env wrapper coming from the gym library.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, devices, discrete, custom_reward, solar, sum_cost, cost_ESU, zeta, stop=None):\n",
    "        # print(schema_filepath)\n",
    "\n",
    "        # new obs\n",
    "        if solar:\n",
    "            self.index_keep = [0,1,2,3,22,23,27]\n",
    "            self.index_norm = [12,7,24,1,1,1,1,1]\n",
    "        else:\n",
    "            self.index_keep = [0,1,2,3,22,27]\n",
    "            self.index_norm = [12,7,24,1,1,1,1]\n",
    "\n",
    "        self.custom_reward = custom_reward\n",
    "        self.sum_cost = sum_cost\n",
    "        self.cost_ESU = cost_ESU\n",
    "        self.zeta = zeta\n",
    "        self.discrete = discrete\n",
    "\n",
    "        #normalization reward\n",
    "        # self.mean_std = (0.7850008976449486, 0.1339831060216876)\n",
    "\n",
    "        self.env = env\n",
    "        #list of names of devices [[]]\n",
    "        self.devices = devices\n",
    "        self.building_devices = []\n",
    "        # get the number of buildings\n",
    "        self.num_buildings = len(self.env.action_space)\n",
    "\n",
    "        low = self.env.observation_space[0].low\n",
    "        high = self.env.observation_space[0].high\n",
    "\n",
    "        #if sum cost\n",
    "        if self.sum_cost:\n",
    "            cost_l = low[19]+low[28]\n",
    "            cost_h = high[19]+high[28]\n",
    "\n",
    "        d_low, d_high = [], []\n",
    "        for i in devices[0]:\n",
    "            if i == 'battery':\n",
    "                d_low.append(low[26])\n",
    "                d_high.append(high[26])\n",
    "            elif i == 'cooling':\n",
    "                d_low.append(low[24])\n",
    "                d_high.append(high[24])\n",
    "            elif i == 'dhw':\n",
    "                d_low.append(low[25])\n",
    "                d_high.append(high[25])\n",
    "\n",
    "        low = [low[i] for i in self.index_keep]\n",
    "        high = [high[i] for i in self.index_keep]\n",
    "\n",
    "        low = low + d_low\n",
    "        high = high + d_high\n",
    "\n",
    "        #if sum cost\n",
    "        if self.sum_cost:\n",
    "            low.append(cost_l)\n",
    "            high.append(cost_h)\n",
    "\n",
    "        #if cost ESU, chage if multiple buildings\n",
    "        if self.cost_ESU:\n",
    "            for i in range(len(devices[0])):\n",
    "                low.append(0)\n",
    "                high.append(cost_h)\n",
    "\n",
    "        if self.discrete:\n",
    "            self.action_space = gym.spaces.Discrete(21)\n",
    "            self.action_map = [-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0,\n",
    "                                0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "        else:\n",
    "            self.action_space = env.action_space[0]\n",
    "        self.observation_space = gym.spaces.Box(low=np.array(low), high=np.array(high), dtype=np.float32)\n",
    "\n",
    "        #keep last outdoor temp for each building\n",
    "        self.temp = []\n",
    "        self.stop = stop\n",
    "        self.rewards = []\n",
    "\n",
    "        #remove if test\n",
    "        self.print_config()\n",
    "\n",
    "        # TO THINK : normalize the observation space\n",
    "\n",
    "    def reset(self):\n",
    "        obs_dict = env_reset(self.env)\n",
    "        obs = self.env.reset()\n",
    "\n",
    "        for i,e in enumerate(self.env.buildings):\n",
    "          self.building_devices.append(BuildingDevices(e,i))\n",
    "        self.temp.append(obs[i][3])\n",
    "        \n",
    "        return self.get_obs(obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        we apply the same action for all the buildings\n",
    "        \"\"\"\n",
    "        t = self.env.time_step\n",
    "        # print('action', action,'\\n')\n",
    "        \n",
    "        #if action is discrete convert using action mapping\n",
    "        if self.discrete:\n",
    "            action = [[self.action_conversion(action)]]\n",
    "            action = action[0]\n",
    "        # print('action', action)\n",
    "        action = [action]\n",
    "\n",
    "        # we do a step in the environment\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        # print('normal_reward', reward)\n",
    "        if t == self.stop:\n",
    "            done = True\n",
    "        \n",
    "        #custom reward 1 is the one where we can use zeta\n",
    "        if self.custom_reward == 1:\n",
    "            for i,e in enumerate(self.env.buildings):\n",
    "                rewards = []\n",
    "                rewards.append(compute_loss(e, self.building_devices[i], self.env.buildings[i].pricing.electricity_pricing[t-1],\n",
    "                self.env.buildings[i].carbon_intensity.carbon_intensity[t-1], self.temp[i], self.zeta))\n",
    "                self.temp[i] = obs[i][3]\n",
    "                #TODO multiple buildings\n",
    "                return np.array(self.get_obs(obs)), -rewards[0], done, info\n",
    "            \n",
    "        # custom reward 2 is cost without storage - cost with storage\n",
    "        elif self.custom_reward == 2:\n",
    "            for i in range(len(self.env.buildings)):\n",
    "                rewards = self.reward_diff(i)\n",
    "                # print('rewards', rewards)\n",
    "                self.rewards.append(rewards)\n",
    "                #TODO multiple buildings\n",
    "                return np.array(self.get_obs(obs)), rewards, done, info\n",
    "            \n",
    "        # custom reward 3 is the same as 1 but coded in a different way (not coded to be used w/ thermal storage)\n",
    "        elif self.custom_reward == 3:\n",
    "            rewards = []\n",
    "            for i in range(len(self.env.buildings)):\n",
    "                #TODO multiple buildings\n",
    "                rewards.append(self.building_devices[i].cost(self.zeta))\n",
    "                # print('reward3', self.building_devices[i].cost(self.zeta))\n",
    "                return np.array(self.get_obs(obs)), rewards[0], done, info  \n",
    "\n",
    "        #else use normal reward \n",
    "        else:\n",
    "            #TODO multiple buildings\n",
    "            return np.array(self.get_obs(obs)), reward[0], done, info\n",
    "\n",
    "    def get_obs(self, obs):\n",
    "        #keep common obs\n",
    "        obs_ = [[o[i]/n for i,n in zip(self.index_keep, self.index_norm)] for o in obs]\n",
    "        # obs_ = list(itertools.chain(*obs_))\n",
    "\n",
    "        #add soc of each device for each building\n",
    "        for o in range(len(obs_)):\n",
    "            if 'battery' in self.devices[o]:\n",
    "                i = obs[o][26]\n",
    "                if isinstance(i, np.ndarray):\n",
    "                    print('probleme array instead of float soc battery obs')\n",
    "                    i = i[0]\n",
    "                obs_[o].append(i)\n",
    "            if 'cooling' in self.devices[o]:\n",
    "                obs_[o].append(obs[o][24])\n",
    "            if 'dhw' in self.devices[o]:\n",
    "                obs_[o].append(obs[o][25])\n",
    "\n",
    "        #add sum of costs (emission+price)\n",
    "        if self.sum_cost is True:\n",
    "            for o in range(len(obs_)):\n",
    "                #modify for buildings that dont have same nb of obs\n",
    "                obs_[o].append(obs[o][19]+obs[o][28])\n",
    "            # print(obs)\n",
    "        \n",
    "        #add cost of energy in storage device for each device of each building\n",
    "        if self.cost_ESU is True:\n",
    "            for o in range(len(obs_)):\n",
    "                for i in self.devices[o]:\n",
    "                    obs_[o].append(self.building_devices[o].devices[i].cost)\n",
    "        return np.array(obs_)\n",
    "    \n",
    "    def action_conversion(self, action):\n",
    "        return self.action_map[action]\n",
    "    \n",
    "    def valid_action_mask(self):\n",
    "        mod_action_space = self.building_devices[0].compute_bounds()\n",
    "        act = np.array(self.action_map)\n",
    "        index = list(np.where((act>mod_action_space.low[0]) & (act<mod_action_space.high[0]))[0])\n",
    "        act = [True if i in index else False for i in range(21)]\n",
    "        act[10] = True #noop always valid\n",
    "        return act\n",
    "    \n",
    "    def print_config(self):\n",
    "        print('INIT ENV:')\n",
    "        act = 'Discrete' if self.discrete else 'Continuous'\n",
    "        print(f'ACTION SPACE: {act}')\n",
    "        print(f'Use of custom reward: {self.custom_reward}')\n",
    "        if self.custom_reward in [1,3]:\n",
    "            print(f'    zeta: {self.zeta}')\n",
    "        print('Observations kept:')\n",
    "        for i in self.index_keep:\n",
    "            print(f'    {i}: {self.env.observation_names[0][i]}')\n",
    "        for i in self.devices[0]:\n",
    "            if i == 'battery':\n",
    "                print('    26: '+self.env.observation_names[0][26])\n",
    "            elif i == 'cooling':\n",
    "                print('    24: '+self.env.observation_names[0][24])\n",
    "            elif i == 'dhw':\n",
    "                print('    25: '+self.env.observation_names[0][25])\n",
    "        if self.sum_cost or self.cost_ESU:\n",
    "            print(f'Observations ADDED:')\n",
    "            if self.sum_cost:\n",
    "                print(f'    sum_cost: {self.env.observation_names[0][19]} + {self.env.observation_names[0][28]}')\n",
    "            if self.cost_ESU:\n",
    "                print('    cost_ESU: see Device.loss')\n",
    "\n",
    "\n",
    "    def reward_diff(self, building_i):\n",
    "        r = []\n",
    "        building = self.env.buildings[building_i]\n",
    "        c1 = building.net_electricity_consumption_cost[-1]\n",
    "        c2 = building.net_electricity_consumption_emission[-1]\n",
    "        c = c1 + c2\n",
    "\n",
    "        # c1_ = building.net_electricity_consumption_without_storage_cost[-1]\n",
    "        # c2_ = building.net_electricity_consumption_without_storage_emission[-1]\n",
    "        # c_ = c1_ + c2_\n",
    "\n",
    "        c1_ = building.net_electricity_consumption_without_storage_and_pv_cost[-1]\n",
    "        c2_ = building.net_electricity_consumption_without_storage_and_pv_emission[-1]\n",
    "        c_ = c1_ + c2_\n",
    "\n",
    "        final_cost = c_ - c\n",
    "        return final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env: gym.Env) -> np.ndarray:\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.valid_action_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_name(model_name, env, total_timesteps):\n",
    "    \"\"\"\n",
    "    get info about training session.\n",
    "    \"\"\"\n",
    "    action_space = 'Discrete' if env.discrete else 'Continuous'\n",
    "    reward = f'customR_{int(env.custom_reward)}'\n",
    "    if env.custom_reward in [1,3]:\n",
    "        reward += f'_zeta_{env.zeta}'\n",
    "    equipment = 'devices'+'-'.join([str(len(i)) for i in env.devices])\n",
    "\n",
    "    p = [model_name, str(env.num_buildings)+'building', equipment, action_space,\n",
    "        reward, 'sum_cost_'+str(int(env.sum_cost)), \n",
    "        'cost_ESU_'+str(int(env.cost_ESU)), str(total_timesteps)]\n",
    "\n",
    "    return '_'.join(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_model(model_name, devices, discrete, custom_reward, solar, sum_cost,\n",
    "                    cost_ESU, zeta, stop, checkpoint_path='./results', total_timesteps=None):\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    if not total_timesteps:\n",
    "        total_timesteps = 1_500_000\n",
    "\n",
    "    if solar:\n",
    "        schema_filepath = 'schema2.json'\n",
    "    else:\n",
    "        schema_filepath = 'schema3.json'\n",
    "    schema = read_json(schema_filepath)\n",
    "    print(schema_filepath)\n",
    "    schema['root_directory'] = './'\n",
    "    env = CityLearnEnv(schema)\n",
    "    env = EnvCityGym(env, devices=devices, discrete=discrete, custom_reward=custom_reward,\n",
    "                        solar=solar, sum_cost=sum_cost, cost_ESU=cost_ESU, zeta=zeta, stop=stop)\n",
    "    if 'mask' in model_name:\n",
    "        env = ActionMasker(env, mask_fn)\n",
    "    obs = np.array(env.reset())\n",
    "\n",
    "    exp_name = get_exp_name(model_name, env,total_timesteps)\n",
    "    # load model if exist\n",
    "    if model_name == 'ppo_mask':\n",
    "        model = MaskablePPO(MaskableActorCriticPolicy, env,\n",
    "                        verbose=1, tensorboard_log='./train', device='cuda')\n",
    "    elif model_name == 'ppo':\n",
    "        model = PPO('MlpPolicy', env, verbose=1, gamma=0.99, tensorboard_log=\"./train/\", device='cuda',\n",
    "                    n_steps=10_000, learning_rate=0.0006, clip_range=0.3, ent_coef=0.00)\n",
    "    else:\n",
    "        print('model not recognized')\n",
    "        return None\n",
    "\n",
    "    print(f'Model: {model_name}')\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=total_timesteps, tb_log_name=exp_name,log_interval=5)\n",
    "\n",
    "    print('saving model')\n",
    "    model.save(checkpoint_path+exp_name+'.zip')\n",
    "    if 'mask' in model_name:\n",
    "        env = env.env\n",
    "    return model, env.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_heuristic(mode):\n",
    "    \"\"\"\n",
    "    For 1 building.\n",
    "    mode:\n",
    "        noop: test an agent that takes no action (action 0)\n",
    "        random: test an agent that takes random action over ection space\n",
    "    \"\"\"\n",
    "    schema_filepath = 'schema2.json'\n",
    "    schema = read_json(schema_filepath)\n",
    "    schema['root_directory'] = './'\n",
    "    env = CityLearnEnv(schema)\n",
    "    env = EnvCityGym(env, devices=[['battery']], discrete=False, custom_reward=True,\n",
    "                    sum_cost=True, cost_ESU=True, zeta=0.1, stop=8760)\n",
    "\n",
    "    \n",
    "    obs = np.array(env.reset())\n",
    "    _ = env.reset()\n",
    "    done = False\n",
    "    action_list = []\n",
    "\n",
    "    while not done:\n",
    "        if mode == 'noop':\n",
    "            action = [0]\n",
    "            _, _, done, _ = env.step(action)\n",
    "        elif mode == 'random':\n",
    "            action = env.action_space[0].sample()\n",
    "            _, _, done, _ = env.step(action)\n",
    "\n",
    "        action_list.append(action[0])\n",
    "            \n",
    "    solar = env.env.buildings[0].energy_simulation.solar_generation\n",
    "    solar = env.env.buildings[0].pv.get_generation(solar)\n",
    "    conso = env.env.buildings[0].net_electricity_consumption\n",
    "    price = env.env.buildings[0].pricing.electricity_pricing\n",
    "    carbon = env.env.buildings[0].carbon_intensity.carbon_intensity\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['Time [hours]'] = [i for i in range(len(conso))]\n",
    "    df['Net conso [kWh]'] = conso\n",
    "    df['SOC [kWh]'] = env.env.buildings[0].electrical_storage.soc\n",
    "    df['Conso w/o storage [kWh]'] = env.env.buildings[0].net_electricity_consumption_without_storage\n",
    "    df['Conso w/o storage and PV [kWh]'] = env.env.buildings[0].net_electricity_consumption_without_storage_and_pv\n",
    "    df['Solar generation [kWh]'] = solar\n",
    "    # df.iloc[0][0] = 24 #first is last day of july\n",
    "    df['Cost sum(emission,price)x50'] = (price+carbon)*50\n",
    "    df['Cost price x100'] = price*100\n",
    "    df['Cost carbon x100'] = carbon*100\n",
    "\n",
    "    return df, action_list, env #all vals of df in kWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, model_path, discrete, custom_reward, solar, sum_cost,\n",
    "                cost_ESU, zeta=0, total_timesteps=None):\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    try:\n",
    "        if model_name == 'ppo':\n",
    "            print('PPO')\n",
    "            model = PPO.load(model_path)\n",
    "        elif model_name == 'ddpg':\n",
    "            print('DDPG')\n",
    "            model = DDPG.load(model_path)\n",
    "        elif model_name == 'a2c':\n",
    "            print('A2C')\n",
    "            model = A2C.load(model_path)\n",
    "        elif model_name == 'sac':\n",
    "            print('SAC')\n",
    "            model = SAC.load(model_path)\n",
    "        elif model_name == 'ppo_mask':\n",
    "            model = MaskablePPO.load(model_path)\n",
    "    except:\n",
    "        print('not_found')\n",
    "\n",
    "    for i in range(1):\n",
    "        done = False\n",
    "        print(f'Case {i}:', i)\n",
    "        schema_filepath = 'schema3.json'\n",
    "        schema = read_json(schema_filepath)\n",
    "        schema['root_directory'] = './'\n",
    "        env = CityLearnEnv(schema)\n",
    "        env = EnvCityGym(env, devices=[['battery']], discrete=discrete, custom_reward=custom_reward,\n",
    "                solar=solar, sum_cost=sum_cost, cost_ESU=cost_ESU, zeta=0, stop=8760)\n",
    "        if 'mask' in model_name:\n",
    "            env = ActionMasker(env, mask_fn)\n",
    "        print(env)\n",
    "        obs = np.array(env.reset())\n",
    "        print()\n",
    "        action_list = []\n",
    "        while not done:\n",
    "            # print(obs)\n",
    "            # obs = [i[0] if isinstance(i, np.array()) else i for i in obs]\n",
    "            # obs = np.array(obs)\n",
    "            # print(obs)\n",
    "            action, _state = model.predict(obs[0], deterministic=True)\n",
    "            # print(type(action))\n",
    "            obs, rewards, done, _ = env.step(action)\n",
    "            if isinstance(action, np.ndarray):\n",
    "                action = int(action)\n",
    "            action_list.append(action)\n",
    "\n",
    "        if discrete:\n",
    "            if 'mask' in model_name:\n",
    "                env = env.env\n",
    "            action_list = [env.action_conversion(i) for i in action_list]\n",
    "                \n",
    "            \n",
    "        # print(action_list)\n",
    "        x = pd.Series(action_list, name='action')\n",
    "        print('List of different actions taken:')\n",
    "        print(x.value_counts())\n",
    "\n",
    "        for n, nd in env.env.evaluate().groupby('name'):\n",
    "            nd = nd.pivot(index='name', columns='cost_function', values='value').round(3)\n",
    "            print(n, ':', nd.to_dict('records'))\n",
    "        print()\n",
    "\n",
    "    solar = env.env.buildings[0].energy_simulation.solar_generation\n",
    "    solar = env.env.buildings[0].pv.get_generation(solar)\n",
    "    conso = env.env.buildings[0].net_electricity_consumption\n",
    "\n",
    "    carbon = env.env.buildings[0].carbon_intensity.carbon_intensity\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    # df['Time [hours]'] = [i for i in range(len(conso))]\n",
    "    df['Net conso [kWh]'] = conso\n",
    "    df['SOC [kWh]'] = env.env.buildings[0].electrical_storage.soc\n",
    "    df['Conso w/o storage [kWh]'] = env.env.buildings[0].net_electricity_consumption_without_storage\n",
    "    df['Conso w/o storage and PV [kWh]'] = env.env.buildings[0].net_electricity_consumption_without_storage_and_pv\n",
    "    df['Solar generation [kWh]'] = solar\n",
    "    # df.iloc[0][0] = 24 #first is last day of july\n",
    "    df['Cost sum(emission,price)x50'] = (price+carbon)*50\n",
    "    df['Cost price x100'] = price*100\n",
    "    df['Cost carbon x100'] = carbon*100\n",
    "\n",
    "    return df, action_list, env #all vals of df in kWh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_plot(df, y_label=None, day_mark=False, show=True, write=False):\n",
    "    \"\"\"\n",
    "    write(str): path/name.html\n",
    "    \"\"\"\n",
    "    df['Time [hours]'] = [i for i in range(len(df))]\n",
    "    fig = px.line(df, x=\"Time [hours]\", y=list(df.columns))\n",
    "    if day_mark:\n",
    "        marks = [hour for hour in df[\"Time [hours]\"] if hour%24==0]\n",
    "        for hour in marks:\n",
    "            fig.add_vline(x=hour)\n",
    "\n",
    "    if y_label:\n",
    "        fig.update_layout(yaxis_title=y_label)\n",
    "    if show:\n",
    "        fig.show()\n",
    "    if write:\n",
    "        print('writing to', write)\n",
    "        fig.write_html(write)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 1\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_filepath = 'schema3.json'\n",
    "schema = read_json(schema_filepath)\n",
    "schema['root_directory'] = './'\n",
    "\n",
    "env = CityLearnEnv(schema)\n",
    "env = EnvCityGym(env, devices=[['battery']], discrete=True, custom_reward=3,\n",
    "                    solar=False, sum_cost=True, cost_ESU=False, zeta=1, stop=24*30*3)\n",
    "_ = env.reset()\n",
    "len(_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema3.json\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 1\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Model: ppo_mask\n",
      "Logging to ./train\\ppo_mask_1building_devices1_Discrete_customR_3_zeta_1_sum_cost_1_cost_ESU_1_3000000_1\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -4.68e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016167455 |\n",
      "|    clip_fraction        | 0.000928     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.11        |\n",
      "|    explained_variance   | 4.98e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.47e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 7.27e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -4.64e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030648543 |\n",
      "|    clip_fraction        | 0.00151      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.09        |\n",
      "|    explained_variance   | 1.29e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.29e+05     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00516     |\n",
      "|    value_loss           | 4.14e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -4.6e+05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 195          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058104377 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.98        |\n",
      "|    explained_variance   | 2.03e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04e+05     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00717     |\n",
      "|    value_loss           | 3.94e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -4.53e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004598634 |\n",
      "|    clip_fraction        | 0.00366     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 3.58e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.51e+05    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00358    |\n",
      "|    value_loss           | 6.1e+05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -4.46e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007963103 |\n",
      "|    clip_fraction        | 0.00117     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.45e+05    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00322    |\n",
      "|    value_loss           | 1.08e+06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -4.26e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004913901 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.18e+05    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0045     |\n",
      "|    value_loss           | 9.46e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -4.17e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 465          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015849881 |\n",
      "|    clip_fraction        | 0.00332      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.485       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.7e+05      |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 3.3e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -4.09e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 154           |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 529           |\n",
      "|    total_timesteps      | 81920         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00083517027 |\n",
      "|    clip_fraction        | 0.00552       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.294        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.94e+04      |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.00194      |\n",
      "|    value_loss           | 1.53e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -4.03e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 155          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 592          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007050383 |\n",
      "|    clip_fraction        | 0.00215      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.02e+05     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 2.01e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.97e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 654          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011613541 |\n",
      "|    clip_fraction        | 0.00732      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.284       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.92e+05     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 4.19e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.93e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 717          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008140727 |\n",
      "|    clip_fraction        | 0.00967      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.75e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 8.47e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.85e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 783           |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2136314e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.351        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.98e+05      |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -0.000353     |\n",
      "|    value_loss           | 7.92e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.82e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 157           |\n",
      "|    iterations           | 65            |\n",
      "|    time_elapsed         | 845           |\n",
      "|    total_timesteps      | 133120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020267215 |\n",
      "|    clip_fraction        | 0.00117       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.159        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.3e+05       |\n",
      "|    n_updates            | 640           |\n",
      "|    policy_gradient_loss | -0.00046      |\n",
      "|    value_loss           | 2.52e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.79e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 158           |\n",
      "|    iterations           | 70            |\n",
      "|    time_elapsed         | 906           |\n",
      "|    total_timesteps      | 143360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011905398 |\n",
      "|    clip_fraction        | 0.00166       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.132        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.61e+04      |\n",
      "|    n_updates            | 690           |\n",
      "|    policy_gradient_loss | -0.000552     |\n",
      "|    value_loss           | 1.21e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.77e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 158           |\n",
      "|    iterations           | 75            |\n",
      "|    time_elapsed         | 968           |\n",
      "|    total_timesteps      | 153600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018800855 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.146        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.81e+04      |\n",
      "|    n_updates            | 740           |\n",
      "|    policy_gradient_loss | -0.000399     |\n",
      "|    value_loss           | 1.76e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.75e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 158           |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 1031          |\n",
      "|    total_timesteps      | 163840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017900372 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.208        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.24e+05      |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | -0.000481     |\n",
      "|    value_loss           | 4.28e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.73e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 157           |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 1102          |\n",
      "|    total_timesteps      | 174080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027184736 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.148        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.66e+05      |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | -0.00049      |\n",
      "|    value_loss           | 8.24e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.7e+05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 1169         |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.623495e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.128       |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.34e+05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000265    |\n",
      "|    value_loss           | 6.83e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.68e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 95            |\n",
      "|    time_elapsed         | 1240          |\n",
      "|    total_timesteps      | 194560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2628566e-05 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0783       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.72e+04      |\n",
      "|    n_updates            | 940           |\n",
      "|    policy_gradient_loss | -0.00031      |\n",
      "|    value_loss           | 1.96e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.67e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 1308         |\n",
      "|    total_timesteps      | 204800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.138154e-05 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0535      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.17e+04     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.000486    |\n",
      "|    value_loss           | 1.03e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.66e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 105           |\n",
      "|    time_elapsed         | 1370          |\n",
      "|    total_timesteps      | 215040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0902294e-05 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0526       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.96e+04      |\n",
      "|    n_updates            | 1040          |\n",
      "|    policy_gradient_loss | -0.000242     |\n",
      "|    value_loss           | 1.51e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.64e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 110          |\n",
      "|    time_elapsed         | 1432         |\n",
      "|    total_timesteps      | 225280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003440904 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0637      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.15e+05     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.000655    |\n",
      "|    value_loss           | 4.26e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.63e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 1500         |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.673689e-05 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0934      |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.68e+05     |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.000304    |\n",
      "|    value_loss           | 7.79e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.61e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 120           |\n",
      "|    time_elapsed         | 1568          |\n",
      "|    total_timesteps      | 245760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017453486 |\n",
      "|    clip_fraction        | 0.00146       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.138        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.8e+05       |\n",
      "|    n_updates            | 1190          |\n",
      "|    policy_gradient_loss | -0.000564     |\n",
      "|    value_loss           | 5.96e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.6e+05      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 125           |\n",
      "|    time_elapsed         | 1636          |\n",
      "|    total_timesteps      | 256000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020341089 |\n",
      "|    clip_fraction        | 0.00112       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0879       |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.32e+04      |\n",
      "|    n_updates            | 1240          |\n",
      "|    policy_gradient_loss | -0.000397     |\n",
      "|    value_loss           | 1.61e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.6e+05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 130          |\n",
      "|    time_elapsed         | 1699         |\n",
      "|    total_timesteps      | 266240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003436202 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0596      |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.42e+04     |\n",
      "|    n_updates            | 1290         |\n",
      "|    policy_gradient_loss | -0.000848    |\n",
      "|    value_loss           | 9.18e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.59e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 157           |\n",
      "|    iterations           | 135           |\n",
      "|    time_elapsed         | 1760          |\n",
      "|    total_timesteps      | 276480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052291097 |\n",
      "|    clip_fraction        | 0.00337       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0751       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.78e+04      |\n",
      "|    n_updates            | 1340          |\n",
      "|    policy_gradient_loss | -0.000794     |\n",
      "|    value_loss           | 1.35e+05      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -3.58e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 1826        |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000391889 |\n",
      "|    clip_fraction        | 0.00342     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.36e+05    |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.000674   |\n",
      "|    value_loss           | 4.18e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.58e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 145          |\n",
      "|    time_elapsed         | 1887         |\n",
      "|    total_timesteps      | 296960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.789782e-05 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.184       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.26e+05     |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.000181    |\n",
      "|    value_loss           | 7.82e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.56e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 157           |\n",
      "|    iterations           | 150           |\n",
      "|    time_elapsed         | 1951          |\n",
      "|    total_timesteps      | 307200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9262766e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.129        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.5e+05       |\n",
      "|    n_updates            | 1490          |\n",
      "|    policy_gradient_loss | -0.000385     |\n",
      "|    value_loss           | 5.23e+05      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 8.76e+03       |\n",
      "|    ep_rew_mean          | -3.56e+05      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 157            |\n",
      "|    iterations           | 155            |\n",
      "|    time_elapsed         | 2011           |\n",
      "|    total_timesteps      | 317440         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000105267056 |\n",
      "|    clip_fraction        | 0.00083        |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0916        |\n",
      "|    explained_variance   | -1.19e-07      |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 5.16e+04       |\n",
      "|    n_updates            | 1540           |\n",
      "|    policy_gradient_loss | -0.00029       |\n",
      "|    value_loss           | 1.33e+05       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.55e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 2076         |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.087281e-05 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0723      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.17e+04     |\n",
      "|    n_updates            | 1590         |\n",
      "|    policy_gradient_loss | -0.000342    |\n",
      "|    value_loss           | 7.93e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.55e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 157           |\n",
      "|    iterations           | 165           |\n",
      "|    time_elapsed         | 2144          |\n",
      "|    total_timesteps      | 337920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00070931297 |\n",
      "|    clip_fraction        | 0.00669       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.109        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.43e+04      |\n",
      "|    n_updates            | 1640          |\n",
      "|    policy_gradient_loss | -0.000903     |\n",
      "|    value_loss           | 1.36e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.54e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 170           |\n",
      "|    time_elapsed         | 2223          |\n",
      "|    total_timesteps      | 348160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029802055 |\n",
      "|    clip_fraction        | 0.000586      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0949       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.85e+05      |\n",
      "|    n_updates            | 1690          |\n",
      "|    policy_gradient_loss | -0.00031      |\n",
      "|    value_loss           | 4.01e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.54e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 155           |\n",
      "|    iterations           | 175           |\n",
      "|    time_elapsed         | 2307          |\n",
      "|    total_timesteps      | 358400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017564907 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.173        |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.26e+05      |\n",
      "|    n_updates            | 1740          |\n",
      "|    policy_gradient_loss | -0.000428     |\n",
      "|    value_loss           | 7.28e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.53e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 180          |\n",
      "|    time_elapsed         | 2382         |\n",
      "|    total_timesteps      | 368640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002940526 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.185       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.58e+05     |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | -0.000203    |\n",
      "|    value_loss           | 4.5e+05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.53e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 2457         |\n",
      "|    total_timesteps      | 378880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028201775 |\n",
      "|    clip_fraction        | 0.00728      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.2e+04      |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.00265     |\n",
      "|    value_loss           | 9.43e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.52e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 153           |\n",
      "|    iterations           | 190           |\n",
      "|    time_elapsed         | 2536          |\n",
      "|    total_timesteps      | 389120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016780257 |\n",
      "|    clip_fraction        | 0.0022        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.119        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.53e+04      |\n",
      "|    n_updates            | 1890          |\n",
      "|    policy_gradient_loss | -0.00032      |\n",
      "|    value_loss           | 6.82e+04      |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.76e+03   |\n",
      "|    ep_rew_mean          | -3.52e+05  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 152        |\n",
      "|    iterations           | 195        |\n",
      "|    time_elapsed         | 2612       |\n",
      "|    total_timesteps      | 399360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00449016 |\n",
      "|    clip_fraction        | 0.0123     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.16      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.67e+04   |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.00201   |\n",
      "|    value_loss           | 1.27e+05   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.52e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 200          |\n",
      "|    time_elapsed         | 2692         |\n",
      "|    total_timesteps      | 409600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017833835 |\n",
      "|    clip_fraction        | 0.00664      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.364       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+05     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 4.27e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.52e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 151          |\n",
      "|    iterations           | 205          |\n",
      "|    time_elapsed         | 2772         |\n",
      "|    total_timesteps      | 419840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016798091 |\n",
      "|    clip_fraction        | 0.00527      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.36e+05     |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 6.9e+05      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -3.51e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 210         |\n",
      "|    time_elapsed         | 2856        |\n",
      "|    total_timesteps      | 430080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002605576 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77e+05    |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.00206    |\n",
      "|    value_loss           | 3.86e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.51e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 149          |\n",
      "|    iterations           | 215          |\n",
      "|    time_elapsed         | 2937         |\n",
      "|    total_timesteps      | 440320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007063407 |\n",
      "|    clip_fraction        | 0.00723      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.67e+04     |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 6.91e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.51e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 149          |\n",
      "|    iterations           | 220          |\n",
      "|    time_elapsed         | 3013         |\n",
      "|    total_timesteps      | 450560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014833326 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.145       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.19e+04     |\n",
      "|    n_updates            | 2190         |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 5.72e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.51e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 149          |\n",
      "|    iterations           | 225          |\n",
      "|    time_elapsed         | 3092         |\n",
      "|    total_timesteps      | 460800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014968996 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.135       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.59e+04     |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 1.23e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.5e+05      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 148           |\n",
      "|    iterations           | 230           |\n",
      "|    time_elapsed         | 3166          |\n",
      "|    total_timesteps      | 471040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035230536 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.145        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.2e+05       |\n",
      "|    n_updates            | 2290          |\n",
      "|    policy_gradient_loss | -0.000594     |\n",
      "|    value_loss           | 3.99e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.5e+05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 148          |\n",
      "|    iterations           | 235          |\n",
      "|    time_elapsed         | 3241         |\n",
      "|    total_timesteps      | 481280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005491143 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.99e+05     |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.000275    |\n",
      "|    value_loss           | 6.17e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.5e+05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 147          |\n",
      "|    iterations           | 240          |\n",
      "|    time_elapsed         | 3326         |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008860633 |\n",
      "|    clip_fraction        | 0.00615      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.182       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.77e+05     |\n",
      "|    n_updates            | 2390         |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 2.99e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.5e+05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 147          |\n",
      "|    iterations           | 245          |\n",
      "|    time_elapsed         | 3412         |\n",
      "|    total_timesteps      | 501760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005242974 |\n",
      "|    clip_fraction        | 0.00684      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.114       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.62e+04     |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 5.89e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.5e+05      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 146           |\n",
      "|    iterations           | 250           |\n",
      "|    time_elapsed         | 3494          |\n",
      "|    total_timesteps      | 512000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060377654 |\n",
      "|    clip_fraction        | 0.00669       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0967       |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.98e+04      |\n",
      "|    n_updates            | 2490          |\n",
      "|    policy_gradient_loss | -0.00129      |\n",
      "|    value_loss           | 5.31e+04      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.49e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 146          |\n",
      "|    iterations           | 255          |\n",
      "|    time_elapsed         | 3564         |\n",
      "|    total_timesteps      | 522240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010770555 |\n",
      "|    clip_fraction        | 0.011        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.248       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.17e+04     |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 1.22e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.49e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 146          |\n",
      "|    iterations           | 260          |\n",
      "|    time_elapsed         | 3644         |\n",
      "|    total_timesteps      | 532480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011213106 |\n",
      "|    clip_fraction        | 0.00566      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.327       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.08e+05     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 4.2e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.49e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 145           |\n",
      "|    iterations           | 265           |\n",
      "|    time_elapsed         | 3723          |\n",
      "|    total_timesteps      | 542720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097493804 |\n",
      "|    clip_fraction        | 0.00366       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.328        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.18e+05      |\n",
      "|    n_updates            | 2640          |\n",
      "|    policy_gradient_loss | -0.00135      |\n",
      "|    value_loss           | 5.9e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.49e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 145           |\n",
      "|    iterations           | 270           |\n",
      "|    time_elapsed         | 3796          |\n",
      "|    total_timesteps      | 552960        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060219114 |\n",
      "|    clip_fraction        | 0.00146       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.177        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+05      |\n",
      "|    n_updates            | 2690          |\n",
      "|    policy_gradient_loss | -0.0007       |\n",
      "|    value_loss           | 2.25e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.49e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 275          |\n",
      "|    time_elapsed         | 3870         |\n",
      "|    total_timesteps      | 563200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025617592 |\n",
      "|    clip_fraction        | 0.00444      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0665      |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.03e+04     |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 4.83e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.49e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 280          |\n",
      "|    time_elapsed         | 3942         |\n",
      "|    total_timesteps      | 573440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035295046 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3e+04      |\n",
      "|    n_updates            | 2790         |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 3.75e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.49e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 285          |\n",
      "|    time_elapsed         | 4014         |\n",
      "|    total_timesteps      | 583680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021398342 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.252       |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.47e+04     |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 1.19e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.49e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 290          |\n",
      "|    time_elapsed         | 4085         |\n",
      "|    total_timesteps      | 593920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012583977 |\n",
      "|    clip_fraction        | 0.0063       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12e+05     |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    value_loss           | 4.06e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.49e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 295          |\n",
      "|    time_elapsed         | 4160         |\n",
      "|    total_timesteps      | 604160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013525265 |\n",
      "|    clip_fraction        | 0.00239      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.28        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.33e+05     |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.000939    |\n",
      "|    value_loss           | 5.34e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.48e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 300          |\n",
      "|    time_elapsed         | 4235         |\n",
      "|    total_timesteps      | 614400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003628369 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.63e+04     |\n",
      "|    n_updates            | 2990         |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    value_loss           | 1.74e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.48e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 305          |\n",
      "|    time_elapsed         | 4304         |\n",
      "|    total_timesteps      | 624640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003395612 |\n",
      "|    clip_fraction        | 0.00503      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0625      |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.7e+04      |\n",
      "|    n_updates            | 3040         |\n",
      "|    policy_gradient_loss | -0.000817    |\n",
      "|    value_loss           | 4.73e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.48e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 145           |\n",
      "|    iterations           | 310           |\n",
      "|    time_elapsed         | 4375          |\n",
      "|    total_timesteps      | 634880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045489927 |\n",
      "|    clip_fraction        | 0.00537       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.1          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.72e+04      |\n",
      "|    n_updates            | 3090          |\n",
      "|    policy_gradient_loss | -0.000753     |\n",
      "|    value_loss           | 3.27e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.48e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 145           |\n",
      "|    iterations           | 315           |\n",
      "|    time_elapsed         | 4446          |\n",
      "|    total_timesteps      | 645120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015106791 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.262        |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.41e+04      |\n",
      "|    n_updates            | 3140          |\n",
      "|    policy_gradient_loss | -0.000291     |\n",
      "|    value_loss           | 1.24e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.48e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 144           |\n",
      "|    iterations           | 320           |\n",
      "|    time_elapsed         | 4525          |\n",
      "|    total_timesteps      | 655360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032612117 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.439        |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.36e+05      |\n",
      "|    n_updates            | 3190          |\n",
      "|    policy_gradient_loss | -0.000693     |\n",
      "|    value_loss           | 4.27e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.48e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 144           |\n",
      "|    iterations           | 325           |\n",
      "|    time_elapsed         | 4592          |\n",
      "|    total_timesteps      | 665600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013298812 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.449        |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.03e+05      |\n",
      "|    n_updates            | 3240          |\n",
      "|    policy_gradient_loss | -0.000201     |\n",
      "|    value_loss           | 4.89e+05      |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.76e+03   |\n",
      "|    ep_rew_mean          | -3.48e+05  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 145        |\n",
      "|    iterations           | 330        |\n",
      "|    time_elapsed         | 4660       |\n",
      "|    total_timesteps      | 675840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00086057 |\n",
      "|    clip_fraction        | 0.000977   |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.332     |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.6e+04    |\n",
      "|    n_updates            | 3290       |\n",
      "|    policy_gradient_loss | -0.00122   |\n",
      "|    value_loss           | 1.47e+05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -3.48e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 335         |\n",
      "|    time_elapsed         | 4726        |\n",
      "|    total_timesteps      | 686080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021410536 |\n",
      "|    clip_fraction        | 0.0707      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35e+04    |\n",
      "|    n_updates            | 3340        |\n",
      "|    policy_gradient_loss | 0.000881    |\n",
      "|    value_loss           | 4.57e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.48e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 340          |\n",
      "|    time_elapsed         | 4792         |\n",
      "|    total_timesteps      | 696320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018116027 |\n",
      "|    clip_fraction        | 0.011        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.22e+04     |\n",
      "|    n_updates            | 3390         |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 3.6e+04      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.48e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 345          |\n",
      "|    time_elapsed         | 4855         |\n",
      "|    total_timesteps      | 706560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003780327 |\n",
      "|    clip_fraction        | 0.00752      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.453       |\n",
      "|    explained_variance   | -4.23e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.52e+04     |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.000538    |\n",
      "|    value_loss           | 1.3e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.48e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 145           |\n",
      "|    iterations           | 350           |\n",
      "|    time_elapsed         | 4918          |\n",
      "|    total_timesteps      | 716800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6852595e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.17         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.65e+05      |\n",
      "|    n_updates            | 3490          |\n",
      "|    policy_gradient_loss | -5.12e-05     |\n",
      "|    value_loss           | 3.65e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.48e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 355          |\n",
      "|    time_elapsed         | 4986         |\n",
      "|    total_timesteps      | 727040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.240418e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.309       |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.83e+05     |\n",
      "|    n_updates            | 3540         |\n",
      "|    policy_gradient_loss | -0.000153    |\n",
      "|    value_loss           | 4.09e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.48e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 360          |\n",
      "|    time_elapsed         | 5055         |\n",
      "|    total_timesteps      | 737280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.659473e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.064       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.94e+04     |\n",
      "|    n_updates            | 3590         |\n",
      "|    policy_gradient_loss | -0.000243    |\n",
      "|    value_loss           | 9.07e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.47e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 145           |\n",
      "|    iterations           | 365           |\n",
      "|    time_elapsed         | 5132          |\n",
      "|    total_timesteps      | 747520        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9888114e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0209       |\n",
      "|    explained_variance   | 4.66e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.6e+04       |\n",
      "|    n_updates            | 3640          |\n",
      "|    policy_gradient_loss | -1.85e-05     |\n",
      "|    value_loss           | 3.99e+04      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.47e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 370          |\n",
      "|    time_elapsed         | 5204         |\n",
      "|    total_timesteps      | 757760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.845557e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0226      |\n",
      "|    explained_variance   | 0.0989       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.53e+04     |\n",
      "|    n_updates            | 3690         |\n",
      "|    policy_gradient_loss | -5.36e-07    |\n",
      "|    value_loss           | 3.12e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.47e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 145           |\n",
      "|    iterations           | 375           |\n",
      "|    time_elapsed         | 5271          |\n",
      "|    total_timesteps      | 768000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2489525e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0257       |\n",
      "|    explained_variance   | 0.139         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.84e+04      |\n",
      "|    n_updates            | 3740          |\n",
      "|    policy_gradient_loss | -9.04e-05     |\n",
      "|    value_loss           | 1.01e+05      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 8.76e+03       |\n",
      "|    ep_rew_mean          | -3.47e+05      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 145            |\n",
      "|    iterations           | 380            |\n",
      "|    time_elapsed         | 5349           |\n",
      "|    total_timesteps      | 778240         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000100573845 |\n",
      "|    clip_fraction        | 0.000586       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.034         |\n",
      "|    explained_variance   | 0.0729         |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 1.72e+05       |\n",
      "|    n_updates            | 3790           |\n",
      "|    policy_gradient_loss | -0.000249      |\n",
      "|    value_loss           | 3.43e+05       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.47e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 385          |\n",
      "|    time_elapsed         | 5421         |\n",
      "|    total_timesteps      | 788480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.090646e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.025       |\n",
      "|    explained_variance   | 0.000411     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.85e+05     |\n",
      "|    n_updates            | 3840         |\n",
      "|    policy_gradient_loss | -5.29e-05    |\n",
      "|    value_loss           | 3.45e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.47e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 390          |\n",
      "|    time_elapsed         | 5491         |\n",
      "|    total_timesteps      | 798720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.300154e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0257      |\n",
      "|    explained_variance   | 0.0279       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+04     |\n",
      "|    n_updates            | 3890         |\n",
      "|    policy_gradient_loss | -4.75e-05    |\n",
      "|    value_loss           | 6.59e+04     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.46e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 145           |\n",
      "|    iterations           | 395           |\n",
      "|    time_elapsed         | 5577          |\n",
      "|    total_timesteps      | 808960        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1744007e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0129       |\n",
      "|    explained_variance   | 0.132         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.49e+04      |\n",
      "|    n_updates            | 3940          |\n",
      "|    policy_gradient_loss | -1.22e-06     |\n",
      "|    value_loss           | 3.73e+04      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 8.76e+03      |\n",
      "|    ep_rew_mean          | -3.46e+05     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 144           |\n",
      "|    iterations           | 400           |\n",
      "|    time_elapsed         | 5651          |\n",
      "|    total_timesteps      | 819200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -9.433592e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0196       |\n",
      "|    explained_variance   | 0.192         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.43e+04      |\n",
      "|    n_updates            | 3990          |\n",
      "|    policy_gradient_loss | -1.23e-06     |\n",
      "|    value_loss           | 2.83e+04      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.46e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 144          |\n",
      "|    iterations           | 405          |\n",
      "|    time_elapsed         | 5728         |\n",
      "|    total_timesteps      | 829440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.303504e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0215      |\n",
      "|    explained_variance   | 0.197        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.11e+04     |\n",
      "|    n_updates            | 4040         |\n",
      "|    policy_gradient_loss | -7.71e-05    |\n",
      "|    value_loss           | 1.08e+05     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 8.76e+03       |\n",
      "|    ep_rew_mean          | -3.46e+05      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 144            |\n",
      "|    iterations           | 410            |\n",
      "|    time_elapsed         | 5811           |\n",
      "|    total_timesteps      | 839680         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000103655235 |\n",
      "|    clip_fraction        | 0.000879       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -0.0261        |\n",
      "|    explained_variance   | 0.0603         |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 1.52e+05       |\n",
      "|    n_updates            | 4090           |\n",
      "|    policy_gradient_loss | -0.000269      |\n",
      "|    value_loss           | 3.5e+05        |\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "m = train_save_model(model_name='ppo_mask', devices=[['battery']], discrete=True,\n",
    "                    custom_reward=3, solar=False, sum_cost=True, cost_ESU=True, zeta=1,\n",
    "                    stop=None, checkpoint_path='./weights/', total_timesteps=3_000_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise training curves in Tensorboard use: CTRL + MAJ + P, Python: Launch Tensorboard, select folder ./train (VSCODE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_found\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vincent\\Documents\\these\\citylearn\\new_citylearn\\discrete.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mppo_mask_1building_devices1_Discrete_customR_3_zeta_1_sum_cost_1_cost_ESU_0_2000000.zip\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df, actions, env \u001b[39m=\u001b[39m test_model(\u001b[39m'\u001b[39;49m\u001b[39mppo_mask\u001b[39;49m\u001b[39m'\u001b[39;49m, model_path\u001b[39m=\u001b[39;49mpath, discrete\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, custom_reward\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, sum_cost\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                 solar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, cost_ESU\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, zeta\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Vincent\\Documents\\these\\citylearn\\new_citylearn\\discrete.ipynb Cell 23\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model_name, model_path, discrete, custom_reward, solar, sum_cost, cost_ESU, zeta, total_timesteps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m action_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# print(obs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# obs = [i[0] if isinstance(i, np.array()) else i for i in obs]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# obs = np.array(obs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# print(obs)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     action, _state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs[\u001b[39m0\u001b[39m], deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# print(type(action))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     obs, rewards, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "path = 'weights\\ppo_mask_1building_devices1_Discrete_customR_3_zeta_1_sum_cost_1_cost_ESU_0_2000000.zip'\n",
    "\n",
    "df, actions, env = test_model('ppo_mask', model_path=path, discrete=True, custom_reward=3, sum_cost=True,\n",
    "                solar=False, cost_ESU=True, zeta=0, total_timesteps=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results of single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to test1.html\n"
     ]
    }
   ],
   "source": [
    "fig_plot(df, day_mark=True, show=False, write='test1.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.1_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0.0    8757\n",
      "0.3       2\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.002, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.2_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8647\n",
      "-0.3     108\n",
      " 0.1       4\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.002, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.3_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8315\n",
      "-0.2     340\n",
      " 0.7      63\n",
      " 0.3      32\n",
      "-0.3       9\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.013, 'cost': 1.012, 'electricity_consumption': 1.013, 'zero_net_energy': 1.003}]\n",
      "District : [{'1 - load_factor': 1.01, 'average_daily_peak': 1.076, 'carbon_emissions': 1.013, 'cost': 1.012, 'electricity_consumption': 1.013, 'peak_demand': 1.0, 'ramping': 1.13, 'zero_net_energy': 1.003}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.4_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8638\n",
      "-0.3     121\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.5_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8484\n",
      "-0.4     175\n",
      " 0.1     100\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.001, 'cost': 1.001, 'electricity_consumption': 1.001, 'zero_net_energy': 1.001}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.001, 'carbon_emissions': 1.001, 'cost': 1.001, 'electricity_consumption': 1.001, 'peak_demand': 1.0, 'ramping': 1.029, 'zero_net_energy': 1.001}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.6_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8757\n",
      "-0.3       2\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.7_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8462\n",
      " 0.1     292\n",
      "-0.4       5\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.001}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.002, 'zero_net_energy': 1.001}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.8_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8538\n",
      "-0.6     124\n",
      "-0.1      88\n",
      "-0.2       7\n",
      " 0.3       2\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.001, 'average_daily_peak': 1.001, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.001, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0.9_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8623\n",
      " 0.1     132\n",
      "-0.2       4\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.001}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.002, 'zero_net_energy': 1.001}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_0_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      " 0.0    8611\n",
      "-0.5     148\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_1_zeta_1_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "-0.1    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_2_zeta_1_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "not_found\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 1\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    23: solar_generation\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vincent\\Documents\\these\\citylearn\\new_citylearn\\discrete.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m i:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mppo_mask\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df, actions, env \u001b[39m=\u001b[39m test_model(model_name, model_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./weights_elec/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mi, discrete\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, custom_reward\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, sum_cost\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 cost_ESU\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, solar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, zeta\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Vincent\\Documents\\these\\citylearn\\new_citylearn\\discrete.ipynb Cell 25\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model_name, model_path, discrete, custom_reward, solar, sum_cost, cost_ESU, zeta, total_timesteps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m action_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# print(obs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# obs = [i[0] if isinstance(i, np.array()) else i for i in obs]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# obs = np.array(obs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# print(obs)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     action, _state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs[\u001b[39m0\u001b[39m], deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# print(type(action))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X31sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     obs, rewards, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(weights_folder):\n",
    "    print(i)\n",
    "    if 'mask' in i:\n",
    "        model_name = 'ppo_mask'\n",
    "\n",
    "        df, actions, env = test_model(model_name, model_path='./weights_elec/'+i, discrete=True, custom_reward=1, sum_cost=True,\n",
    "                    cost_ESU=True, solar=True, zeta=0, total_timesteps=None)\n",
    "        \n",
    "        #print fig here\n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_solar(env):\n",
    "    df_solar = pd.DataFrame()\n",
    "    for i,b in enumerate(env.buildings):\n",
    "        solar = b.energy_simulation.solar_generation\n",
    "        solar = b.pv.get_generation(solar)\n",
    "        df_solar['building_'+str(i)] = solar\n",
    "    return df_solar\n",
    "\n",
    "def data_nsl(env):\n",
    "    df = pd.DataFrame()\n",
    "    for i,b in enumerate(env.buildings):\n",
    "        x = b.energy_simulation.non_shiftable_load\n",
    "        df['building_'+str(i)] = x\n",
    "    return df\n",
    "\n",
    "def data_cool(env):\n",
    "    df = pd.DataFrame()\n",
    "    for i,b in enumerate(env.buildings):\n",
    "        x = b.energy_simulation.cooling_demand\n",
    "        df['building_'+str(i)] = x\n",
    "    return df\n",
    "\n",
    "def data_dhw(env):\n",
    "    df = pd.DataFrame()\n",
    "    for i,b in enumerate(env.buildings):\n",
    "        x = b.energy_simulation.dhw_demand\n",
    "        df['building_'+str(i)] = x\n",
    "    return df\n",
    "\n",
    "def data_cost(env):\n",
    "    #cost is same for each b\n",
    "    df_cost = pd.DataFrame()\n",
    "    price = env.buildings[0].pricing.electricity_pricing\n",
    "    # price = pd.read_csv('./pricing.csv')['Electricity Pricing [$]']*100\n",
    "    carbon = env.buildings[0].carbon_intensity.carbon_intensity\n",
    "\n",
    "    df_cost['Cost sum(emission,price)x50'] = (price+carbon)*50\n",
    "    df_cost['Cost price x100'] = price*100\n",
    "    df_cost['Cost carbon x100'] = carbon*100\n",
    "\n",
    "    return df_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at replay buffer actions during training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
