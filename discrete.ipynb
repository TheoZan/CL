{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info\n",
    "\n",
    "* discrete action space of size 21, 10 discharge, 10 charge, 1 noop\n",
    "* only One building\n",
    "* only one battery\n",
    "* can use custom reward with zeta parameter\n",
    "* can yous masked action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "import citylearn\n",
    "from citylearn.energy_model import HeatPump\n",
    "from citylearn.utilities import read_json\n",
    "\n",
    "import gym\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from stable_baselines3 import PPO, A2C, DDPG, TD3, SAC\n",
    "\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    building_info = env.get_building_information()\n",
    "    building_info = list(building_info)\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "                \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingDevices:\n",
    "  \"\"\"\n",
    "    Keeps track of all storage devices of a building.\n",
    "  \"\"\"\n",
    "  def __init__(self, building, num_building):\n",
    "    self.num_building = num_building\n",
    "    self.building = building\n",
    "    self.devices = {'battery' : Device_(building.electrical_storage, 'battery'),\n",
    "                    'cooling' : None,\n",
    "                    'dhw' : None}\n",
    "    \n",
    "  def compute_bounds(self):\n",
    "    bounds = [self.bounds_action(i) for i,j in self.devices.items() if j is not None]\n",
    "    return gym.spaces.Box(low=np.array([i[0] for i in bounds]), high=np.array([i[1] for i in bounds]), dtype=np.float64)\n",
    "  \n",
    "# ACTION 0 :  cooling\n",
    "# ACTION 1 : dhw\n",
    "# ACTION 2 : battery\n",
    "  \n",
    "  def bounds_action(self, type_action):\n",
    "    device = self.devices[type_action].device\n",
    "    if device is None:\n",
    "        return None # if return none building doest have battery\n",
    "    if type_action == 'battery':\n",
    "        capacity = device.capacity_history[-2] if len(device.capacity_history) > 1 else device.capacity\n",
    "        #HIGH\n",
    "        #get max energy that the storage unit can use to charge [kW]\n",
    "        #if trying to put more than the battery can accept reject action\n",
    "        high1 = device.get_max_input_power()/capacity\n",
    "        high2 = (device.capacity - device.soc_init)/(0.95*device.capacity) #approxim (efficiency = 0.95)\n",
    "        high = min(high1, high2, 1)\n",
    "\n",
    "        #LOW\n",
    "        low1 = -device.get_max_input_power()/capacity\n",
    "        low2 = (-device.soc_init*0.95)/device.capacity #approxim (efficiency = 0.95)\n",
    "        low = max(low1, low2, -1)\n",
    "\n",
    "    else:\n",
    "        bool_h2, bool_l2 = False, False\n",
    "        if type_action == 'cooling':\n",
    "            # print('\\ncooling')\n",
    "            space_demand = self.building.cooling_demand[self.building.time_step]\n",
    "            max_output = self.building.cooling_device.get_max_output_power(self.building.weather.outdoor_dry_bulb_temperature[self.building.time_step], False)\n",
    "            # print('space_demand',space_demand)\n",
    "            # print('max_output', max_output)\n",
    "            # print('capacity:', device.capacity)\n",
    "        else: #dhw\n",
    "            # print('\\ndhw')\n",
    "            space_demand = self.building.dhw_demand[self.building.time_step]\n",
    "            max_output = self.building.dhw_device.get_max_output_power(self.building.weather.outdoor_dry_bulb_temperature[self.building.time_step], False)\\\n",
    "            if isinstance(self.building.dhw_device, HeatPump) else self.building.dhw_device.get_max_output_power()\n",
    "            # print('space_demand',space_demand)\n",
    "            # print('max_output', max_output)\n",
    "        space_demand = 0 if space_demand is None or math.isnan(space_demand) else space_demand # case where space demand is unknown\n",
    "\n",
    "        #HIGH\n",
    "        high1 = (max_output-space_demand) / device.capacity\n",
    "        # print('high1', high1)\n",
    "        if device.max_input_power is not None:\n",
    "            bool_h2 = True\n",
    "            high2 = device.max_input_power / device.capacity\n",
    "            # print('high2', high2)\n",
    "        high3 = (device.capacity - device.soc_init) / (device.capacity*device.efficiency)\n",
    "        # print(device.capacity, device.soc_init)\n",
    "        # print('high3', high3)\n",
    "        \n",
    "        if bool_h2:\n",
    "            high = min(high1, high2, high3, 0.5)\n",
    "        else:\n",
    "            high = min(high1, high3, 0.5)\n",
    "\n",
    "\n",
    "        #LOW\n",
    "        low1 = -space_demand / device.capacity\n",
    "        # print('low1', low1)\n",
    "        if device.max_output_power is not None:\n",
    "            bool_l2 = True\n",
    "            low2 = -device.max_output_power / device.capacity\n",
    "            # print('low2',low2)\n",
    "        low3 = (-device.soc_init*device.efficiency) / device.capacity\n",
    "        # print('low3',low3)\n",
    "\n",
    "        if bool_l2:\n",
    "            low = max(low1, low2, low3, -0.5)\n",
    "        else:\n",
    "            low = max(low1, low3, -0.5)\n",
    "\n",
    "    return (low, high)\n",
    "  \n",
    "  def cost(self, zeta):\n",
    "    \"\"\"\n",
    "    Other way to compute cost.\n",
    "    1) we compute the total electrical consumption of the building,\n",
    "    2) we the offset the PV generation if existant.\n",
    "    3) we treat the case of charging and discharging the device \n",
    "    \"\"\"\n",
    "    #without dhw and cooling storage\n",
    "    #net conso = cooling + dhw + electrical_storage + nsl - solar\n",
    "    global_conso = 0\n",
    "    building = self.building\n",
    "\n",
    "    price = building.pricing.electricity_pricing[building.time_step]\n",
    "    carbon = building.carbon_intensity.carbon_intensity[building.time_step]\n",
    "\n",
    "    # print(building.time_step)\n",
    "    cooling_demand = building.energy_simulation.cooling_demand[building.time_step] + building.cooling_storage.energy_balance[building.time_step]\n",
    "    cooling_conso = building.cooling_device.get_input_power(cooling_demand, building.weather.outdoor_dry_bulb_temperature[building.time_step], heating=False)\n",
    "    global_conso += cooling_conso\n",
    "\n",
    "    dhw_demand = building.energy_simulation.dhw_demand[building.time_step] + building.dhw_storage.energy_balance[building.time_step]\n",
    "    if isinstance(building.dhw_device, HeatPump):\n",
    "            dhw_consumption = building.dhw_device.get_input_power(dhw_demand, building.weather.outdoor_dry_bulb_temperature[building.time_step], heating=True)\n",
    "    else:\n",
    "            dhw_consumption = building.dhw_device.get_input_power(dhw_demand)\n",
    "    \n",
    "    global_conso += dhw_consumption\n",
    "    global_conso += building.energy_simulation.non_shiftable_load[building.time_step]\n",
    "    global_conso -= building.pv.get_generation(building.energy_simulation.solar_generation)[building.time_step]\n",
    "\n",
    "    # print('globa_conso', global_conso)\n",
    "\n",
    "    #battery\n",
    "    #discharge \n",
    "    #energy that can be used by building (< energy actually discharged)\n",
    "    battery_conso_used = building.energy_from_electrical_storage[building.time_step]\n",
    "    #remove from global conso the energy delivered by battery (not bought from the grid)\n",
    "    # print('battery_conso_used', battery_conso_used)\n",
    "    global_conso -= battery_conso_used\n",
    "\n",
    "    #energy coming out of battery\n",
    "    soc_t = building.electrical_storage.soc[-1]\n",
    "    soc_t_1 = 0 if len(building.electrical_storage.soc) < 2 else building.electrical_storage.soc[-2]\n",
    "    battery_net_conso =  max(0, soc_t_1 - soc_t) #keep only the case where we discharge\n",
    "    # print('battery_net_conso', battery_net_conso)\n",
    "    adjusted_battery_net_conso = battery_net_conso * (1 - zeta)\n",
    "    \n",
    "    #charge\n",
    "    energy_used = building.energy_to_electrical_storage[building.time_step]\n",
    "    global_conso += (energy_used * zeta)\n",
    "    if energy_used > 0: #charging\n",
    "        #update cost of 1 unit of energy in the device\n",
    "        self.devices['battery'].update_cost(energy_used, price, carbon)\n",
    "    \n",
    "    # print('globa_conso', global_conso)\n",
    "    #can be neagtive\n",
    "    global_conso = max(0, global_conso)\n",
    "\n",
    "    # print(global_conso)\n",
    "    cost = (price + carbon) * global_conso\n",
    "    cost += self.devices['battery'].cost * adjusted_battery_net_conso\n",
    "\n",
    "    return -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Device_:\n",
    "  def __init__(self, device, storage_type):\n",
    "    self.device = device\n",
    "    # self.price_cost = 0\n",
    "    # self.emission_cost = 0\n",
    "    self.cost = 0\n",
    "    self.storage_type = storage_type\n",
    "\n",
    "  def loss(self, cost_t, pv_offset, battery_offset):\n",
    "    \"\"\"\n",
    "    get avg price between (battery release, grid release and PV- direct consumption)\n",
    "    add relative incertainty, but true in pratice as the energy is added up in a global consumption pool \n",
    "\n",
    "    battery: if battery releases, price = avg((total released by battery - remaining conso), grid) in the case of thermal\n",
    "    in the case of battery, avg price with PV\n",
    "    \"\"\"\n",
    "    if not self.device:\n",
    "      print('not device')\n",
    "      raise ValueError\n",
    "\n",
    "    energy_used = self.device.energy_balance[-1]\n",
    "    if isinstance(energy_used, np.ndarray):\n",
    "      print('probleme energy used array instead of float')\n",
    "      energy_used = energy_used[0]\n",
    "\n",
    "    #charge\n",
    "    if energy_used > 0:\n",
    "      #if pv production, part of the energy is free\n",
    "      if pv_offset > 0:\n",
    "        energy_used = max(0, energy_used-pv_offset)\n",
    "      #if usage of battery, part of energy has been already taken into account so free\n",
    "      if battery_offset > 0:\n",
    "        energy_used = max(0, energy_used-battery_offset)\n",
    "      # self.price_cost = ((self.price_cost*self.device.soc[-2])+(energy_used*price))/self.device.soc[-1]\n",
    "      # self.emission_cost = ((self.emission_cost*self.device.soc[-2])+(energy_used*emission))/self.device.soc[-1]\n",
    "      \n",
    "      total = self.device.soc[-1]\n",
    "      if isinstance(total, np.ndarray):\n",
    "        print('probleme soc-1 array instead of float')\n",
    "        total = total[0]\n",
    "\n",
    "      prev = self.device.soc[-2]\n",
    "      if isinstance(prev, np.ndarray):\n",
    "        print('probleme soc-2 array instead of float')\n",
    "        prev = prev[0]\n",
    "\n",
    "      self.cost = ((self.cost*prev) + (energy_used*cost_t)) / total\n",
    "      return energy_used, None, None #energy_used > 0\n",
    "\n",
    "    #discharge\n",
    "    else:\n",
    "      #energy_processed is total energy used during charge/discharge process including losses\n",
    "      #energy_used is the energy_processed minus the losses (used by building)\n",
    "      energy_processed = self.device.soc[-2]-self.device.soc[-1]\n",
    "      return -energy_used, energy_processed, self.cost # -energy_used > 0, energy_processed > 0 \n",
    "\n",
    "  def update_cost(self, energy_used, price_t, emission_t):\n",
    "    prev_soc = 0 if len(self.device.soc)<2 else self.device.soc[-2]\n",
    "    cost_t = price_t + emission_t\n",
    "    self.cost = ((self.cost*prev_soc) + (energy_used*cost_t)) / self.device.soc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset(building, mode):\n",
    "  \"\"\"\n",
    "  building is env.buildings[i]:\n",
    "  mode = 'pv' or 'battery'\n",
    "\n",
    "  each conso gets an equally distributed offset based on solar generation or battery\n",
    "  discharge\n",
    "  \"\"\"\n",
    "  if mode == 'pv':\n",
    "    if not building.solar_generation is None:\n",
    "      return 0\n",
    "    demands = [building.non_shiftable_load_demand[-2], building.electrical_storage.energy_balance[-1],\n",
    "             building.dhw_demand[-2], building.dhw_storage.energy_balance[-1],\n",
    "             building.cooling_demand[-2], building.cooling_storage.energy_balance[-1]]\n",
    "    count = len([i for i in demands if i > 0])\n",
    "    return -building.solar_generation[-2]/count\n",
    "  else:\n",
    "    if not building.solar_generation is None:\n",
    "      return 0\n",
    "    if building.electrical_storage.energy_balance[-1] >=0:\n",
    "      return 0\n",
    "    demands = [building.non_shiftable_load_demand[-2], building.dhw_demand[-2],\n",
    "            building.dhw_storage.energy_balance[-1], building.cooling_demand[-2],\n",
    "             building.cooling_storage.energy_balance[-1]]\n",
    "    count = len([i for i in demands if i > 0])\n",
    "    return -building.electrical_storage.energy_balance[-1]/count\n",
    "\n",
    "def compute_loss(building, building_devices, price, emission, outdoor_dry_bulb_temperature, zeta):\n",
    "  loss = 0\n",
    "  pv_offset = get_offset(building, 'pv') \n",
    "  battery_offset = get_offset(building, 'battery')\n",
    "  # print('pv offset',pv_offset)\n",
    "  # print('battery_offset', battery_offset)\n",
    "\n",
    "  #1) compute loss for storage devices use or update cost in storage\n",
    "  for name,device in building_devices.devices.items():\n",
    "    #if the device exists in building\n",
    "    if device:\n",
    "      energy_used, energy_processed, cost = device.loss(price*emission, pv_offset, battery_offset)\n",
    "    #else consider it exists and set energy used = 0\n",
    "    #so we can compute the remaining demand associated with the device\n",
    "    else:\n",
    "      energy_used = 0\n",
    "\n",
    "    if not energy_processed: #charge\n",
    "      #account for a part of the cost at charging time\n",
    "      loss += (price * emission) * (energy_used * zeta)\n",
    "    else: #discharge\n",
    "      loss += cost * (energy_processed * (1 - zeta))\n",
    "\n",
    "    #2) compute remaining thermal demand and add cost of remaining direct demand to answer\n",
    "    if name == 'cooling':\n",
    "      #cooling and dhw stored energy is thermal not electrical\n",
    "      remaining = building.cooling_demand[-2] - energy_used\n",
    "      # print('remaining', remaining)\n",
    "      if remaining > 0:\n",
    "        energy = max(0, building.cooling_device.get_input_power(remaining, outdoor_dry_bulb_temperature, False) - pv_offset - battery_offset)\n",
    "        # print('energy', energy)\n",
    "        loss += (price + emission) * energy\n",
    "\n",
    "    elif name == 'dhw':\n",
    "      remaining = building.dhw_demand[-2]\n",
    "      # print('remaining', remaining)\n",
    "      if remaining > 0:\n",
    "        energy = max(0, building.dhw_device.get_input_power(remaining) - pv_offset - battery_offset)\n",
    "        # print('energy', energy)\n",
    "        loss += (price + emission) * energy\n",
    "\n",
    "  #3) compute additionnal loss coming from nsl\n",
    "  nsl = max(0, building.non_shiftable_load_demand[-2] - pv_offset - battery_offset)\n",
    "  loss += (price + emission) * nsl\n",
    "  # print(loss)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvCityGym(gym.Env):\n",
    "    \"\"\"\n",
    "    Env wrapper coming from the gym library.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, devices, discrete, custom_reward, solar, sum_cost, cost_ESU, zeta, stop=None):\n",
    "        # print(schema_filepath)\n",
    "\n",
    "        # new obs\n",
    "        if solar:\n",
    "            self.index_keep = [0,1,2,3,22,23,27]\n",
    "            self.index_norm = [12,7,24,1,1,1,1,1]\n",
    "        else:\n",
    "            self.index_keep = [0,1,2,3,22,27]\n",
    "            self.index_norm = [12,7,24,1,1,1,1]\n",
    "\n",
    "        self.custom_reward = custom_reward\n",
    "        self.sum_cost = sum_cost\n",
    "        self.cost_ESU = cost_ESU\n",
    "        self.zeta = zeta\n",
    "        self.discrete = discrete\n",
    "\n",
    "        #normalization reward\n",
    "        # self.mean_std = (0.7850008976449486, 0.1339831060216876)\n",
    "\n",
    "        self.env = env\n",
    "        #list of names of devices [[]]\n",
    "        self.devices = devices\n",
    "        self.building_devices = []\n",
    "        # get the number of buildings\n",
    "        self.num_buildings = len(self.env.action_space)\n",
    "\n",
    "        low = self.env.observation_space[0].low\n",
    "        high = self.env.observation_space[0].high\n",
    "\n",
    "        #if sum cost\n",
    "        if self.sum_cost:\n",
    "            cost_l = low[19]+low[28]\n",
    "            cost_h = high[19]+high[28]\n",
    "\n",
    "        d_low, d_high = [], []\n",
    "        for i in devices[0]:\n",
    "            if i == 'battery':\n",
    "                d_low.append(low[26])\n",
    "                d_high.append(high[26])\n",
    "            elif i == 'cooling':\n",
    "                d_low.append(low[24])\n",
    "                d_high.append(high[24])\n",
    "            elif i == 'dhw':\n",
    "                d_low.append(low[25])\n",
    "                d_high.append(high[25])\n",
    "\n",
    "        low = [low[i] for i in self.index_keep]\n",
    "        high = [high[i] for i in self.index_keep]\n",
    "\n",
    "        low = low + d_low\n",
    "        high = high + d_high\n",
    "\n",
    "        #if sum cost\n",
    "        if self.sum_cost:\n",
    "            low.append(cost_l)\n",
    "            high.append(cost_h)\n",
    "\n",
    "        #if cost ESU, chage if multiple buildings\n",
    "        if self.cost_ESU:\n",
    "            for i in range(len(devices[0])):\n",
    "                low.append(0)\n",
    "                high.append(cost_h)\n",
    "\n",
    "        if self.discrete:\n",
    "            self.action_space = gym.spaces.Discrete(21)\n",
    "            self.action_map = [-1,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0,\n",
    "                                0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "        else:\n",
    "            self.action_space = env.action_space[0]\n",
    "        self.observation_space = gym.spaces.Box(low=np.array(low), high=np.array(high), dtype=np.float32)\n",
    "\n",
    "        #keep last outdoor temp for each building\n",
    "        self.temp = []\n",
    "        self.stop = stop\n",
    "        self.rewards = []\n",
    "\n",
    "        #remove if test\n",
    "        self.print_config()\n",
    "\n",
    "        # TO THINK : normalize the observation space\n",
    "\n",
    "    def reset(self):\n",
    "        obs_dict = env_reset(self.env)\n",
    "        obs = self.env.reset()\n",
    "\n",
    "        for i,e in enumerate(self.env.buildings):\n",
    "          self.building_devices.append(BuildingDevices(e,i))\n",
    "        self.temp.append(obs[i][3])\n",
    "        \n",
    "        return self.get_obs(obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        we apply the same action for all the buildings\n",
    "        \"\"\"\n",
    "        t = self.env.time_step\n",
    "        # print('action', action,'\\n')\n",
    "        \n",
    "        #if action is discrete convert using action mapping\n",
    "        if self.discrete:\n",
    "            action = [[self.action_conversion(action)]]\n",
    "            action = action[0]\n",
    "        # print('action', action)\n",
    "        action = [action]\n",
    "\n",
    "        # we do a step in the environment\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        # print('normal_reward', reward)\n",
    "        if t == self.stop:\n",
    "            done = True\n",
    "        \n",
    "        #custom reward 1 is the one where we can use zeta\n",
    "        if self.custom_reward == 1:\n",
    "            for i,e in enumerate(self.env.buildings):\n",
    "                rewards = []\n",
    "                rewards.append(compute_loss(e, self.building_devices[i], self.env.buildings[i].pricing.electricity_pricing[t-1],\n",
    "                self.env.buildings[i].carbon_intensity.carbon_intensity[t-1], self.temp[i], self.zeta))\n",
    "                self.temp[i] = obs[i][3]\n",
    "                #TODO multiple buildings\n",
    "                return np.array(self.get_obs(obs)), -rewards[0], done, info\n",
    "            \n",
    "        # custom reward 2 is cost without storage - cost with storage\n",
    "        elif self.custom_reward == 2:\n",
    "            for i in range(len(self.env.buildings)):\n",
    "                rewards = self.reward_diff(i)\n",
    "                # print('rewards', rewards)\n",
    "                self.rewards.append(rewards)\n",
    "                #TODO multiple buildings\n",
    "                return np.array(self.get_obs(obs)), rewards, done, info\n",
    "            \n",
    "        # custom reward 3 is the same as 1 but coded in a different way (not coded to be used w/ thermal storage)\n",
    "        elif self.custom_reward == 3:\n",
    "            rewards = []\n",
    "            for i in range(len(self.env.buildings)):\n",
    "                #TODO multiple buildings\n",
    "                rewards.append(self.building_devices[i].cost(self.zeta))\n",
    "                # print('reward3', self.building_devices[i].cost(self.zeta))\n",
    "                return np.array(self.get_obs(obs)), rewards[0], done, info  \n",
    "\n",
    "        #else use normal reward \n",
    "        else:\n",
    "            #TODO multiple buildings\n",
    "            return np.array(self.get_obs(obs)), reward[0], done, info\n",
    "\n",
    "    def get_obs(self, obs):\n",
    "        #keep common obs\n",
    "        obs_ = [[o[i]/n for i,n in zip(self.index_keep, self.index_norm)] for o in obs]\n",
    "        # obs_ = list(itertools.chain(*obs_))\n",
    "\n",
    "        #add soc of each device for each building\n",
    "        for o in range(len(obs_)):\n",
    "            if 'battery' in self.devices[o]:\n",
    "                i = obs[o][26]\n",
    "                if isinstance(i, np.ndarray):\n",
    "                    print('probleme array instead of float soc battery obs')\n",
    "                    i = i[0]\n",
    "                obs_[o].append(i)\n",
    "            if 'cooling' in self.devices[o]:\n",
    "                obs_[o].append(obs[o][24])\n",
    "            if 'dhw' in self.devices[o]:\n",
    "                obs_[o].append(obs[o][25])\n",
    "\n",
    "        #add sum of costs (emission+price)\n",
    "        if self.sum_cost is True:\n",
    "            for o in range(len(obs_)):\n",
    "                #modify for buildings that dont have same nb of obs\n",
    "                obs_[o].append(obs[o][19]+obs[o][28])\n",
    "            # print(obs)\n",
    "        \n",
    "        #add cost of energy in storage device for each device of each building\n",
    "        if self.cost_ESU is True:\n",
    "            for o in range(len(obs_)):\n",
    "                for i in self.devices[o]:\n",
    "                    obs_[o].append(self.building_devices[o].devices[i].cost)\n",
    "        return np.array(obs_)\n",
    "    \n",
    "    def action_conversion(self, action):\n",
    "        return self.action_map[action]\n",
    "    \n",
    "    def valid_action_mask(self):\n",
    "        mod_action_space = self.building_devices[0].compute_bounds()\n",
    "        act = np.array(self.action_map)\n",
    "        index = list(np.where((act>mod_action_space.low[0]) & (act<mod_action_space.high[0]))[0])\n",
    "        act = [True if i in index else False for i in range(21)]\n",
    "        act[10] = True #noop always valid\n",
    "        return act\n",
    "    \n",
    "    def print_config(self):\n",
    "        print('INIT ENV:')\n",
    "        act = 'Discrete' if self.discrete else 'Continuous'\n",
    "        print(f'ACTION SPACE: {act}')\n",
    "        print(f'Use of custom reward: {self.custom_reward}')\n",
    "        if self.custom_reward in [1,3]:\n",
    "            print(f'    zeta: {self.zeta}')\n",
    "        print('Observations kept:')\n",
    "        for i in self.index_keep:\n",
    "            print(f'    {i}: {self.env.observation_names[0][i]}')\n",
    "        for i in self.devices[0]:\n",
    "            if i == 'battery':\n",
    "                print('    26: '+self.env.observation_names[0][26])\n",
    "            elif i == 'cooling':\n",
    "                print('    24: '+self.env.observation_names[0][24])\n",
    "            elif i == 'dhw':\n",
    "                print('    25: '+self.env.observation_names[0][25])\n",
    "        if self.sum_cost or self.cost_ESU:\n",
    "            print(f'Observations ADDED:')\n",
    "            if self.sum_cost:\n",
    "                print(f'    sum_cost: {self.env.observation_names[0][19]} + {self.env.observation_names[0][28]}')\n",
    "            if self.cost_ESU:\n",
    "                print('    cost_ESU: see Device.loss')\n",
    "\n",
    "\n",
    "    def reward_diff(self, building_i):\n",
    "        r = []\n",
    "        building = self.env.buildings[building_i]\n",
    "        c1 = building.net_electricity_consumption_cost[-1]\n",
    "        c2 = building.net_electricity_consumption_emission[-1]\n",
    "        c = c1 + c2\n",
    "\n",
    "        # c1_ = building.net_electricity_consumption_without_storage_cost[-1]\n",
    "        # c2_ = building.net_electricity_consumption_without_storage_emission[-1]\n",
    "        # c_ = c1_ + c2_\n",
    "\n",
    "        c1_ = building.net_electricity_consumption_without_storage_and_pv_cost[-1]\n",
    "        c2_ = building.net_electricity_consumption_without_storage_and_pv_emission[-1]\n",
    "        c_ = c1_ + c2_\n",
    "\n",
    "        final_cost = c_ - c\n",
    "        return final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env: gym.Env) -> np.ndarray:\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.valid_action_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_name(model_name, env, total_timesteps):\n",
    "    \"\"\"\n",
    "    get info about training session.\n",
    "    \"\"\"\n",
    "    action_space = 'Discrete' if env.discrete else 'Continuous'\n",
    "    reward = f'customR_{int(env.custom_reward)}'\n",
    "    if env.custom_reward in [1,3]:\n",
    "        reward += f'_zeta_{env.zeta}'\n",
    "    equipment = 'devices'+'-'.join([str(len(i)) for i in env.devices])\n",
    "\n",
    "    p = [model_name, str(env.num_buildings)+'building', equipment, action_space,\n",
    "        reward, 'sum_cost_'+str(int(env.sum_cost)), \n",
    "        'cost_ESU_'+str(int(env.cost_ESU)), str(total_timesteps)]\n",
    "\n",
    "    return '_'.join(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_model(model_name, devices, discrete, custom_reward, solar, sum_cost,\n",
    "                    cost_ESU, zeta, stop, checkpoint_path='./results', total_timesteps=None):\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    if not total_timesteps:\n",
    "        total_timesteps = 1_500_000\n",
    "\n",
    "    if solar:\n",
    "        schema_filepath = 'schema2.json'\n",
    "    else:\n",
    "        schema_filepath = 'schema3.json'\n",
    "    schema = read_json(schema_filepath)\n",
    "    print(schema_filepath)\n",
    "    schema['root_directory'] = './'\n",
    "    env = CityLearnEnv(schema)\n",
    "    env = EnvCityGym(env, devices=devices, discrete=discrete, custom_reward=custom_reward,\n",
    "                        solar=solar, sum_cost=sum_cost, cost_ESU=cost_ESU, zeta=zeta, stop=stop)\n",
    "    if 'mask' in model_name:\n",
    "        env = ActionMasker(env, mask_fn)\n",
    "    obs = np.array(env.reset())\n",
    "\n",
    "    exp_name = get_exp_name(model_name, env,total_timesteps)\n",
    "    # load model if exist\n",
    "    if model_name == 'ppo_mask':\n",
    "        model = MaskablePPO(MaskableActorCriticPolicy, env,\n",
    "                        verbose=1, tensorboard_log='./train', device='cuda')\n",
    "    elif model_name == 'ppo':\n",
    "        model = PPO('MlpPolicy', env, verbose=0, gamma=0.99, tensorboard_log=\"./train/\", device='cuda',\n",
    "                    n_steps=10_000, learning_rate=0.0006, clip_range=0.3, ent_coef=0.001)\n",
    "    else:\n",
    "        print('model not recognized')\n",
    "        return None\n",
    "\n",
    "    print(f'Model: {model_name}')\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=total_timesteps, tb_log_name=exp_name,log_interval=5)\n",
    "\n",
    "    print('saving model')\n",
    "    model.save(checkpoint_path+exp_name+'.zip')\n",
    "    if 'mask' in model_name:\n",
    "        env = env.env\n",
    "    return model, env.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_heuristic(mode):\n",
    "    \"\"\"\n",
    "    For 1 building.\n",
    "    mode:\n",
    "        noop: test an agent that takes no action (action 0)\n",
    "        random: test an agent that takes random action over ection space\n",
    "    \"\"\"\n",
    "    schema_filepath = 'schema2.json'\n",
    "    schema = read_json(schema_filepath)\n",
    "    schema['root_directory'] = './'\n",
    "    env = CityLearnEnv(schema)\n",
    "    env = EnvCityGym(env, devices=[['battery']], discrete=False, custom_reward=True,\n",
    "                    sum_cost=True, cost_ESU=True, zeta=0.1, stop=8760)\n",
    "\n",
    "    \n",
    "    obs = np.array(env.reset())\n",
    "    _ = env.reset()\n",
    "    done = False\n",
    "    action_list = []\n",
    "\n",
    "    while not done:\n",
    "        if mode == 'noop':\n",
    "            action = [0]\n",
    "            _, _, done, _ = env.step(action)\n",
    "        elif mode == 'random':\n",
    "            action = env.action_space[0].sample()\n",
    "            _, _, done, _ = env.step(action)\n",
    "\n",
    "        action_list.append(action[0])\n",
    "            \n",
    "    solar = env.env.buildings[0].energy_simulation.solar_generation\n",
    "    solar = env.env.buildings[0].pv.get_generation(solar)\n",
    "    conso = env.env.buildings[0].net_electricity_consumption\n",
    "    price = env.env.buildings[0].pricing.electricity_pricing\n",
    "    carbon = env.env.buildings[0].carbon_intensity.carbon_intensity\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['Time [hours]'] = [i for i in range(len(conso))]\n",
    "    df['Net conso [kWh]'] = conso\n",
    "    df['SOC [kWh]'] = env.env.buildings[0].electrical_storage.soc\n",
    "    df['Conso w/o storage [kWh]'] = env.env.buildings[0].net_electricity_consumption_without_storage\n",
    "    df['Conso w/o storage and PV [kWh]'] = env.env.buildings[0].net_electricity_consumption_without_storage_and_pv\n",
    "    df['Solar generation [kWh]'] = solar\n",
    "    # df.iloc[0][0] = 24 #first is last day of july\n",
    "    df['Cost sum(emission,price)x50'] = (price+carbon)*50\n",
    "    df['Cost price x100'] = price*100\n",
    "    df['Cost carbon x100'] = carbon*100\n",
    "\n",
    "    return df, action_list, env #all vals of df in kWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, model_path, discrete, custom_reward, solar, sum_cost,\n",
    "                cost_ESU, zeta=0, total_timesteps=None):\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    try:\n",
    "        if model_name == 'ppo':\n",
    "            print('PPO')\n",
    "            model = PPO.load(model_path)\n",
    "        elif model_name == 'ddpg':\n",
    "            print('DDPG')\n",
    "            model = DDPG.load(model_path)\n",
    "        elif model_name == 'a2c':\n",
    "            print('A2C')\n",
    "            model = A2C.load(model_path)\n",
    "        elif model_name == 'sac':\n",
    "            print('SAC')\n",
    "            model = SAC.load(model_path)\n",
    "        elif model_name == 'ppo_mask':\n",
    "            model = MaskablePPO.load(model_path)\n",
    "    except:\n",
    "        print('not_found')\n",
    "\n",
    "    for i in range(1):\n",
    "        done = False\n",
    "        print(f'Case {i}:', i)\n",
    "        schema_filepath = 'schema3.json'\n",
    "        schema = read_json(schema_filepath)\n",
    "        schema['root_directory'] = './'\n",
    "        env = CityLearnEnv(schema)\n",
    "        env = EnvCityGym(env, devices=[['battery']], discrete=discrete, custom_reward=custom_reward,\n",
    "                solar=solar, sum_cost=sum_cost, cost_ESU=cost_ESU, zeta=0, stop=8760)\n",
    "        if 'mask' in model_name:\n",
    "            env = ActionMasker(env, mask_fn)\n",
    "        print(env)\n",
    "        obs = np.array(env.reset())\n",
    "        print()\n",
    "        action_list = []\n",
    "        while not done:\n",
    "            # print(obs)\n",
    "            # obs = [i[0] if isinstance(i, np.array()) else i for i in obs]\n",
    "            # obs = np.array(obs)\n",
    "            # print(obs)\n",
    "            action, _state = model.predict(obs[0], deterministic=True)\n",
    "            # print(type(action))\n",
    "            obs, rewards, done, _ = env.step(action)\n",
    "            if isinstance(action, np.ndarray):\n",
    "                action = int(action)\n",
    "            action_list.append(action)\n",
    "\n",
    "        if discrete:\n",
    "            if 'mask' in model_name:\n",
    "                env = env.env\n",
    "            action_list = [env.action_conversion(i) for i in action_list]\n",
    "                \n",
    "            \n",
    "        # print(action_list)\n",
    "        x = pd.Series(action_list, name='action')\n",
    "        print('List of different actions taken:')\n",
    "        print(x.value_counts())\n",
    "\n",
    "        for n, nd in env.env.evaluate().groupby('name'):\n",
    "            nd = nd.pivot(index='name', columns='cost_function', values='value').round(3)\n",
    "            print(n, ':', nd.to_dict('records'))\n",
    "        print()\n",
    "\n",
    "    solar = env.env.buildings[0].energy_simulation.solar_generation\n",
    "    solar = env.env.buildings[0].pv.get_generation(solar)\n",
    "    conso = env.env.buildings[0].net_electricity_consumption\n",
    "    price = env.env.buildings[0].pricing.electricity_pricing\n",
    "    carbon = env.env.buildings[0].carbon_intensity.carbon_intensity\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    # df['Time [hours]'] = [i for i in range(len(conso))]\n",
    "    df['Net conso [kWh]'] = conso\n",
    "    df['SOC [kWh]'] = env.env.buildings[0].electrical_storage.soc\n",
    "    df['Conso w/o storage [kWh]'] = env.env.buildings[0].net_electricity_consumption_without_storage\n",
    "    df['Conso w/o storage and PV [kWh]'] = env.env.buildings[0].net_electricity_consumption_without_storage_and_pv\n",
    "    df['Solar generation [kWh]'] = solar\n",
    "    # df.iloc[0][0] = 24 #first is last day of july\n",
    "    df['Cost sum(emission,price)x50'] = (price+carbon)*50\n",
    "    df['Cost price x100'] = price*100\n",
    "    df['Cost carbon x100'] = carbon*100\n",
    "\n",
    "    return df, action_list, env #all vals of df in kWh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_plot(df, y_label=None, day_mark=False, show=True, write=False):\n",
    "    \"\"\"\n",
    "    write(str): path/name.html\n",
    "    \"\"\"\n",
    "    df['Time [hours]'] = [i for i in range(len(df))]\n",
    "    fig = px.line(df, x=\"Time [hours]\", y=list(df.columns))\n",
    "    if day_mark:\n",
    "        marks = [hour for hour in df[\"Time [hours]\"] if hour%24==0]\n",
    "        for hour in marks:\n",
    "            fig.add_vline(x=hour)\n",
    "\n",
    "    if y_label:\n",
    "        fig.update_layout(yaxis_title=y_label)\n",
    "    if show:\n",
    "        fig.show()\n",
    "    if write:\n",
    "        print('writing to', write)\n",
    "        fig.write_html(write)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 1\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_filepath = 'schema3.json'\n",
    "schema = read_json(schema_filepath)\n",
    "schema['root_directory'] = './'\n",
    "\n",
    "env = CityLearnEnv(schema)\n",
    "env = EnvCityGym(env, devices=[['battery']], discrete=True, custom_reward=3,\n",
    "                    solar=False, sum_cost=True, cost_ESU=False, zeta=1, stop=24*30*3)\n",
    "_ = env.reset()\n",
    "len(_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globa_conso 9.08\n",
      "battery_conso_used -0.0\n",
      "battery_net_conso 0\n",
      "globa_conso 9.08\n"
     ]
    }
   ],
   "source": [
    "_ = env.step(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zeta=0 is only at discharged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema3.json\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 1\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Model: ppo_mask\n",
      "Logging to ./train\\ppo_mask_1building_devices1_Discrete_customR_3_zeta_1_sum_cost_1_cost_ESU_1_2500000_1\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -3.92e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006472802 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 3.82e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.27e+05    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    value_loss           | 4.99e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.88e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 149          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067948196 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.96        |\n",
      "|    explained_variance   | 7.03e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+05     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00671     |\n",
      "|    value_loss           | 2.51e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.84e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 144          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048804246 |\n",
      "|    clip_fraction        | 0.00825      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.86        |\n",
      "|    explained_variance   | 1.79e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+05     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00559     |\n",
      "|    value_loss           | 2.72e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.82e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 144          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 284          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042351307 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.76        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.84e+05     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00343     |\n",
      "|    value_loss           | 4.76e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -3.79e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 358         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011081817 |\n",
      "|    clip_fraction        | 0.0247      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.62e+05    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 9.29e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.72e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 142          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 430          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011459292 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.09e+05     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 9.04e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.69e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 500          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019585784 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.881       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.67e+05     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00443     |\n",
      "|    value_loss           | 3.25e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.66e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 144          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 568          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010856397 |\n",
      "|    clip_fraction        | 0.00229      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.686       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.2e+04      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 1.54e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.64e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 641          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028333596 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.799       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.76e+04     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00462     |\n",
      "|    value_loss           | 2.03e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.62e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 715          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019104716 |\n",
      "|    clip_fraction        | 0.00127      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.641       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.1e+05      |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 4.2e+05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.6e+05     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 785          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017006821 |\n",
      "|    clip_fraction        | 0.00918      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.615       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.21e+05     |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 8.38e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.76e+03    |\n",
      "|    ep_rew_mean          | -3.57e+05   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 863         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001449474 |\n",
      "|    clip_fraction        | 0.00205     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.91e+05    |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    value_loss           | 7.75e+05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.56e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 141          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 940          |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018921912 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.21e+05     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.000998    |\n",
      "|    value_loss           | 2.51e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.76e+03     |\n",
      "|    ep_rew_mean          | -3.55e+05    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 1017         |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007388832 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.63e+04     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 1.19e+05     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "m = train_save_model(model_name='ppo_mask', devices=[['battery']], discrete=True,\n",
    "                    custom_reward=3, solar=False, sum_cost=True, cost_ESU=True, zeta=1,\n",
    "                    stop=None, checkpoint_path='./weights/', total_timesteps=2_500_000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise training curves in Tensorboard use: CTRL + MAJ + P, Python: Launch Tensorboard, select folder ./train (VSCODE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Single model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints:\n",
    "  * name of model\n",
    "  * config of the env\n",
    "  * list and number of different actions taken by env\n",
    "  * list of metrics w/ associated costA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vincent\\Documents\\these\\citylearn\\new_citylearn\\discrete.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mweights_server\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mppo_mask_1building_devices1_Discrete_customR_3_zeta_0_sum_cost_1_cost_ESU_1_3000000.zip\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df, actions, env \u001b[39m=\u001b[39m test_model(\u001b[39m'\u001b[39;49m\u001b[39mppo_mask\u001b[39;49m\u001b[39m'\u001b[39;49m, model_path\u001b[39m=\u001b[39;49mpath, discrete\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, custom_reward\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, sum_cost\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                 solar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, cost_ESU\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, zeta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Vincent\\Documents\\these\\citylearn\\new_citylearn\\discrete.ipynb Cell 24\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model_name, model_path, discrete, custom_reward, solar, sum_cost, cost_ESU, zeta, total_timesteps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m action_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# print(obs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# obs = [i[0] if isinstance(i, np.array()) else i for i in obs]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m# obs = np.array(obs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# print(obs)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     action, _state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(obs[\u001b[39m0\u001b[39;49m], deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# print(type(action))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/Documents/these/citylearn/new_citylearn/discrete.ipynb#X32sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     obs, rewards, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sb3_contrib\\ppo_mask\\ppo_mask.py:379\u001b[0m, in \u001b[0;36mMaskablePPO.predict\u001b[1;34m(self, observation, state, episode_start, deterministic, action_masks)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    359\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    360\u001b[0m     observation: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    364\u001b[0m     action_masks: Optional[np\u001b[39m.\u001b[39mndarray] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    365\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[0;32m    366\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic, action_masks\u001b[39m=\u001b[39;49maction_masks)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sb3_contrib\\common\\maskable\\policies.py:276\u001b[0m, in \u001b[0;36mMaskableActorCriticPolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic, action_masks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_training_mode(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 276\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_to_tensor(observation)\n\u001b[0;32m    278\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    279\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(observation, deterministic\u001b[39m=\u001b[39mdeterministic, action_masks\u001b[39m=\u001b[39maction_masks)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\policies.py:267\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[39m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape))\n\u001b[1;32m--> 267\u001b[0m observation \u001b[39m=\u001b[39m obs_as_tensor(observation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    268\u001b[0m \u001b[39mreturn\u001b[39;00m observation, vectorized_env\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\utils.py:484\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[1;34m(obs, device)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mMoves the observation to the given device.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39m:return: PyTorch tensor of the observation on a desired device.\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m th\u001b[39m.\u001b[39;49mas_tensor(obs, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    485\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    486\u001b[0m     \u001b[39mreturn\u001b[39;00m {key: th\u001b[39m.\u001b[39mas_tensor(_obs, device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m (key, _obs) \u001b[39min\u001b[39;00m obs\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = 'weights_server\\ppo_mask_1building_devices1_Discrete_customR_3_zeta_0_sum_cost_1_cost_ESU_1_3000000.zip'\n",
    "\n",
    "df, actions, env = test_model('ppo_mask', model_path=path, discrete=True, custom_reward=3, sum_cost=True,\n",
    "                solar=False, cost_ESU=True, zeta=1, total_timesteps=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results of single test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to test1.html\n"
     ]
    }
   ],
   "source": [
    "fig_plot(df, day_mark=True, show=False, write='test1.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch of models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints:\n",
    "  * name of model\n",
    "  * config of the env\n",
    "  * list and number of different actions taken by env\n",
    "  * list of metrics w/ associated costA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.1_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.2_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.3_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.4_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.5_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.6_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.7_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.8_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0.9_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_0_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n",
      "ppo_mask_1building_devices1_Discrete_customR_3_zeta_1_sum_cost_1_cost_ESU_1_3000000.zip\n",
      "Case 0: 0\n",
      "INIT ENV:\n",
      "ACTION SPACE: Discrete\n",
      "Use of custom reward: 3\n",
      "    zeta: 0\n",
      "Observations kept:\n",
      "    0: month\n",
      "    1: day_type\n",
      "    2: hour\n",
      "    3: outdoor_dry_bulb_temperature\n",
      "    22: non_shiftable_load\n",
      "    27: net_electricity_consumption\n",
      "    26: electrical_storage_soc\n",
      "Observations ADDED:\n",
      "    sum_cost: carbon_intensity + electricity_pricing\n",
      "    cost_ESU: see Device.loss\n",
      "<ActionMasker<EnvCityGym instance>>\n",
      "\n",
      "List of different actions taken:\n",
      "0    8759\n",
      "Name: action, dtype: int64\n",
      "Building_1 : [{'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'zero_net_energy': 1.0}]\n",
      "District : [{'1 - load_factor': 1.0, 'average_daily_peak': 1.0, 'carbon_emissions': 1.0, 'cost': 1.0, 'electricity_consumption': 1.0, 'peak_demand': 1.0, 'ramping': 1.0, 'zero_net_energy': 1.0}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights_folder = './weights_server/'\n",
    "\n",
    "for i in os.listdir(weights_folder):\n",
    "    print(i)\n",
    "    if 'mask' in i:\n",
    "        model_name = 'ppo_mask'\n",
    "\n",
    "        df, actions, env = test_model(model_name, model_path=weights_folder+i, discrete=True, custom_reward=3, sum_cost=True,\n",
    "                    cost_ESU=True, solar=False, zeta=0, total_timesteps=None)\n",
    "        \n",
    "        #print fig here\n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_solar(env):\n",
    "    df_solar = pd.DataFrame()\n",
    "    for i,b in enumerate(env.buildings):\n",
    "        solar = b.energy_simulation.solar_generation\n",
    "        solar = b.pv.get_generation(solar)\n",
    "        df_solar['building_'+str(i)] = solar\n",
    "    return df_solar\n",
    "\n",
    "def data_nsl(env):\n",
    "    df = pd.DataFrame()\n",
    "    for i,b in enumerate(env.buildings):\n",
    "        x = b.energy_simulation.non_shiftable_load\n",
    "        df['building_'+str(i)] = x\n",
    "    return df\n",
    "\n",
    "def data_cool(env):\n",
    "    df = pd.DataFrame()\n",
    "    for i,b in enumerate(env.buildings):\n",
    "        x = b.energy_simulation.cooling_demand\n",
    "        df['building_'+str(i)] = x\n",
    "    return df\n",
    "\n",
    "def data_dhw(env):\n",
    "    df = pd.DataFrame()\n",
    "    for i,b in enumerate(env.buildings):\n",
    "        x = b.energy_simulation.dhw_demand\n",
    "        df['building_'+str(i)] = x\n",
    "    return df\n",
    "\n",
    "def data_cost(env):\n",
    "    #cost is same for each b\n",
    "    df_cost = pd.DataFrame()\n",
    "    price = env.buildings[0].pricing.electricity_pricing\n",
    "    # price = pd.read_csv('./pricing.csv')['Electricity Pricing [$]']*100\n",
    "    carbon = env.buildings[0].carbon_intensity.carbon_intensity\n",
    "\n",
    "    df_cost['Cost sum(emission,price)x50'] = (price+carbon)*50\n",
    "    df_cost['Cost price x100'] = price*100\n",
    "    df_cost['Cost carbon x100'] = carbon*100\n",
    "\n",
    "    return df_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at replay buffer actions during training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
